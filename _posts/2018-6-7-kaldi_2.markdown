---
layout: post
title:  Kaldi（二）
category: AI 
---

# 数据格式

## text

`utt_id    WORD1 WORD2 WORD3 WORD4 ...`

{% highlight text %}
110236_20091006_82330_F_0001 I'M WORRIED ABOUT THAT
110236_20091006_82330_F_0002 AT LEAST NOW WE HAVE THE BENEFIT
110236_20091006_82330_F_0003 DID YOU EVER GO ON STRIKE
{% endhighlight %}

## segments

`utt_id    file_id    start_time    end_time`

{% highlight text %}
110236_20091006_82330_F_001 110236_20091006_82330_F 0.0 3.44
110236_20091006_82330_F_002 110236_20091006_82330_F 4.60 8.54
110236_20091006_82330_F_003 110236_20091006_82330_F 9.45 12.05
110236_20091006_82330_F_004 110236_20091006_82330_F 13.29 16.13
{% endhighlight %}

## wav.scp

`file_id    path/file`

{% highlight text %}
110236_20091006_82330_F path/110236_20091006_82330_F.wav
111138_20091215_82636_F path/111138_20091215_82636_F.wav
111138_20091217_82636_F path/111138_20091217_82636_F.wav
{% endhighlight %}

## utt2spk

`utt_id    spkr_id`

{% highlight text %}
110236_20091006_82330_F_0001 110236
110236_20091006_82330_F_0002 110236
110236_20091006_82330_F_0003 110236
{% endhighlight %}

## spk2utt

`spkr_id    utt_id1 utt_id2 utt_id3`

## lexicon.txt

lexicon.txt就是发声词典。

{% highlight text %}
WORD        W ER D
LEXICON     L EH K S IH K AH N
{% endhighlight %}

# 对齐工具

Montreal-Forced-Aligner：

https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner

Penn Forced Aligner：

https://web.sas.upenn.edu/phonetics-lab/

FAVE：

http://fave.ling.upenn.edu

# nnet

2010年以后，DL算法逐渐取代传统算法，成为目前的主流。因此，传统部分的代码已经基本稳定，而DL部分还在不断更新中。

kaldi包含了三个不同的DL实现：

nnet1：最早的一个实现，由Karel Vesely维护，因此又叫做Karel's DNN。这个版本只支持单GPU训练，因此修改起来比较简单。nnet1还使用了早期DL常用的pre-training步骤，这在目前基本已经废弃了。

nnet2：nnet1的加强版，由Daniel Povey维护，又叫做Dan's DNN。这个版本支持多GPU、多GPU多线程，而且这些GPU可以不在一台PC上。

kaldi的nnet1和nnet2是以层设计为基础的，也即当你新增加一种神经网络层时需要自己定义它的结构，都有哪些变量，正向怎么算，反向误差怎么传播等等，并且过于复杂的连接方式很难支持。

nnet1和nnet2的模型文件的格式是不兼容的。可以使用steps/nnet2/convert_nnet1_to_nnet2.sh将nnet1的模型文件转换成nnet2的模型文件。

而kaldi的nnet3和CNTK以及TensorFlow都是以图结构为基础的，通过配置文件实现对网络连接方式的定义，数据就像流水一样在你定义的网络图中游走，并自己实现误差的反向传播。

kaldi的DL实现，**不仅具有inference的能力，也具有train的能力**。

kaldi中cnn的例程较少，而且其最新的cnn实现单元TimeHeightConvolutionComponent与机器视觉那边的cnn实现有着很大的区别, 如果按照机器视觉中的cnn实现去做语音识别，那么训练的计算复杂度太高；kaldi最初的cnn实现单元ConvolutionComponent的设计思路和机器视觉cnn实现的思路是一致的，但是由于计算复杂度太高，现在已经打算废弃。

值得注意的是，除了TensorFlow提供的Speech Commands Datasets之外，其他的数据集基本都是以一句话作为训练数据的最小单元。由于训练数据存在着语音和文字之间的帧对齐问题，因此即便是DL方案，通常也需要利用由传统方法生成的粗糙模型，进行相关的对齐操作。

# online

online识别通常会通过麦克风来获取音频，这部分一般是系统函数调用获取得到音频数据，一般系统采用16k采样率，16bits，单通道的音频。当然也可能会用到高采样率等，但对于识别来说已经足够。

kaldi里的在线识别有2个版本，online跟online2。

online是很早的一些版本，通过麦克风获取数据，然后得到文本结果，但只支持gmm的模型。

online2版本没有麦克风获取数据这部分，就直接是音频文件到识别结果，这里支持nnet2跟nnet3的模型。

参考：

https://blog.csdn.net/lijin6249/article/details/51838936

基于kaldi的在线中文识别，online的操作介绍

https://mp.weixin.qq.com/s/Scq8LumtTisPAdRE4CYPdA

kaldi里的在线识别

# 数据增强

数据集的基础数据往往是单一的，我们怎么才能让数据变多呢？

目前在基础数据上增加数据的方法主要有：加性噪声，乘性噪声，便音量，变语速等。这四种方法kaldi里都有对应的脚本干这些事情。

1.加性噪声。比如不同风格的纯音乐和歌曲，不同场景的环境噪声。加冲击响应或者加性噪声的脚本位于egs/aspire/s5/local/multi_condition

2.乘性噪声。不同场景的信道情况，混响，衰减等因素。

3.变音量跟变语速：

音量脚本：egs/wsj/s5/utils/data/perturb_data_dir_volume.sh

语速脚本：egs/wsj/s5/utils/data/perturb_data_dir_speed_3way.sh

kaldi的变音量跟变语速都是借助sox这个工具来实现的，具体的命令大家可以参考脚本，音量的变化范围从1.0/8到2；语速的变化范围就0.9,1,1.1。

# WFST



