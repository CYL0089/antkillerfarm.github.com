---
layout: post
title:  Kaldi（二）
category: AI 
---

# ARPA文件格式（续）

类似地，可以获得三元词组的概率计算公式：

$$P(C│AB)=\left\{\begin{matrix}P(C\mid AB) \qquad ABC存在
\\ \alpha(AB)*P(C\mid B) \qquad BC存在
\\ \alpha(AB)*\alpha(B)*P(C) \qquad 其他
\end{matrix}\right.$$

参考：

https://blog.csdn.net/lv_xinmy/article/details/8595561

ARPA的n-gram语言模型格式

https://blog.csdn.net/SAJIAHAN/article/details/52901422

语言模型-ARPA格式

https://blog.csdn.net/yutianzuijin/article/details/78756130

arpa2fst原理详解

https://www.jianshu.com/p/ab356b3c889e

Kaldi(A5)语言模型及HCLG.fst生成

https://blog.csdn.net/nihaomafb/article/details/48009695

语言模型Katz backoff以及HMM模型

# WFST

## 生成WFST

生成L：

{% highlight bash %}
scripts/make_lexicon_fst.pl data/lexicon.txt 0.5 SIL | \
  fstcompile --isymbols=data/phones.txt --osymbols=data/words.txt \
  --keep_isymbols=false --keep_osymbols=false | \
   fstarcsort --sort_type=olabel > data/L.fst
{% endhighlight %}

生成G：

{% highlight bash %}
gunzip -c data_prep/lm.arpa.gz | \
  arpa2fst --disambig-symbol=#0 \
             --read-symbol-table=data/words.txt - data/G.fst
{% endhighlight %}

生成$$L \circ G$$：

{% highlight bash %}
fsttablecompose data/L_disambig.fst data/G.fst | \
    fstdeterminizestar --use-log=true | \
    fstminimizeencoded | fstpushspecial | \
    fstarcsort --sort-type=ilabel > somedir/LG.fst
{% endhighlight %}

生成$$C \circ L \circ G$$：

{% highlight bash %}
fstmakecontextfst --read-disambig-syms=$dir/disambig_phones.list \
--write-disambig-syms=$dir/disambig_ilabels.list data/phones.txt $subseq_sym \
  $dir/ilabels | fstarcsort --sort_type=olabel > $dir/C.fst
fstaddsubsequentialloop $subseq_sym $dir/LG.fst | \
 fsttablecompose $dir/C.fst - > $dir/CLG.fst
{% endhighlight %}

生成$$H \circ C \circ L \circ G$$：

{% highlight bash %}
make-h-transducer --disambig-syms-out=$dir/disambig_tstate.list \
   --transition-scale=1.0  $dir/ilabels.remapped \
   $tree $model  > $dir/Ha.fst
fsttablecompose $dir/Ha.fst $dir/CLG2.fst | \
   fstdeterminizestar --use-log=true | \
   fstrmsymbols $dir/disambig_tstate.list | \
   fstrmepslocal  | fstminimizeencoded > $dir/HCLGa.fst
add-self-loops --self-loop-scale=0.1 \
    --reorder=true $model < $dir/HCLGa.fst > $dir/HCLG.fst
{% endhighlight %}

## 解码

# 对齐工具

Montreal-Forced-Aligner：

https://github.com/MontrealCorpusTools/Montreal-Forced-Aligner

Penn Forced Aligner：

https://web.sas.upenn.edu/phonetics-lab/

FAVE：

http://fave.ling.upenn.edu

# nnet

2010年以后，DL算法逐渐取代传统算法，成为目前的主流。因此，传统部分的代码已经基本稳定，而DL部分还在不断更新中。

kaldi包含了三个不同的DL实现：

nnet1：最早的一个实现，由Karel Vesely维护，因此又叫做Karel's DNN。这个版本只支持单GPU训练，因此修改起来比较简单。nnet1还使用了早期DL常用的pre-training步骤，这在目前基本已经废弃了。

nnet2：nnet1的加强版，由Daniel Povey维护，又叫做Dan's DNN。这个版本支持多GPU、多GPU多线程，而且这些GPU可以不在一台PC上。

kaldi的nnet1和nnet2是以层设计为基础的，也即当你新增加一种神经网络层时需要自己定义它的结构，都有哪些变量，正向怎么算，反向误差怎么传播等等，并且过于复杂的连接方式很难支持。

nnet1和nnet2的模型文件的格式是不兼容的。可以使用steps/nnet2/convert_nnet1_to_nnet2.sh将nnet1的模型文件转换成nnet2的模型文件。

而kaldi的nnet3和CNTK以及TensorFlow都是以图结构为基础的，通过配置文件实现对网络连接方式的定义，数据就像流水一样在你定义的网络图中游走，并自己实现误差的反向传播。

kaldi的DL实现，**不仅具有inference的能力，也具有train的能力**。

kaldi中cnn的例程较少，而且其最新的cnn实现单元TimeHeightConvolutionComponent与机器视觉那边的cnn实现有着很大的区别, 如果按照机器视觉中的cnn实现去做语音识别，那么训练的计算复杂度太高；kaldi最初的cnn实现单元ConvolutionComponent的设计思路和机器视觉cnn实现的思路是一致的，但是由于计算复杂度太高，现在已经打算废弃。

值得注意的是，除了TensorFlow提供的Speech Commands Datasets之外，其他的数据集基本都是以一句话作为训练数据的最小单元。由于训练数据存在着语音和文字之间的帧对齐问题，因此即便是DL方案，通常也需要利用由传统方法生成的粗糙模型，进行相关的对齐操作。

# Chain

Chain是Kaldi的作者Daniel Povey新进引入的技术，该工作可以看做是对CTC的进一步扩展，直接使用句子级区分性准则进行模型的训练，该方法被认为是下一步提升语音识别效率与性能最有潜力的技术之一。

论文：

《Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI》

参考：

http://www.cnblogs.com/JarvanWang/p/7499589.html

Kaldi中的Chain模型

# online

online识别通常会通过麦克风来获取音频，这部分一般是系统函数调用获取得到音频数据，一般系统采用16k采样率，16bits，单通道的音频。当然也可能会用到高采样率等，但对于识别来说已经足够。

kaldi里的在线识别有2个版本，online跟online2。

online是很早的一些版本，通过麦克风获取数据，然后得到文本结果，但只支持gmm的模型。

online2版本没有麦克风获取数据这部分，就直接是音频文件到识别结果，这里支持nnet2跟nnet3的模型。

参考：

https://blog.csdn.net/lijin6249/article/details/51838936

基于kaldi的在线中文识别，online的操作介绍

https://mp.weixin.qq.com/s/Scq8LumtTisPAdRE4CYPdA

kaldi里的在线识别

# 数据增强

数据集的基础数据往往是单一的，我们怎么才能让数据变多呢？

目前在基础数据上增加数据的方法主要有：加性噪声，乘性噪声，便音量，变语速等。这四种方法kaldi里都有对应的脚本干这些事情。

1.加性噪声。比如不同风格的纯音乐和歌曲，不同场景的环境噪声。加冲击响应或者加性噪声的脚本位于egs/aspire/s5/local/multi_condition

2.乘性噪声。不同场景的信道情况，混响，衰减等因素。

3.变音量跟变语速：

音量脚本：egs/wsj/s5/utils/data/perturb_data_dir_volume.sh

语速脚本：egs/wsj/s5/utils/data/perturb_data_dir_speed_3way.sh

kaldi的变音量跟变语速都是借助sox这个工具来实现的，具体的命令大家可以参考脚本，音量的变化范围从1.0/8到2；语速的变化范围就0.9,1,1.1。

参考：

https://mp.weixin.qq.com/s/p1mAfM8LBLMQILeNOpbf6Q

改进语音识别性能的数据增强技巧

# 使用训练好的模型

由于Kaldi的设计目标主要是方便研究人员训练模型，而不是使用模型，因此很多例子都只有训练步骤，而没有使用步骤。

但实际上，Kaldi官网已经有些训练好的模型可供下载：

http://kaldi-asr.org/models.html

这里比较重要的是CVTE开源的中文模型。

这些训练好的模型的脚本，就基本全是使用指南了。

参考：

http://www.luojie1987.com/index.php/post/140.html

Kaldi离线在线解码应用

## 参考

https://mp.weixin.qq.com/s/xBbYHMQhxkRQ-dy-0gHvaw

如何用kaldi参加京东金融对话语音识别大赛

# 世说新语（续）

## 2018.9

![](/images/img2/blitzkrieg_vs_deep_battle.png)

----

https://mp.weixin.qq.com/s?__biz=MzU5MTM3MzI0Mg==&mid=2247483683&idx=1&sn=8e964074eb5cdced42adefae08a19fdc

古人怎么称呼不同的朝代

----

17-19世纪，东非印度洋沿岸有一个阿曼苏丹国，是海上强国……强到什么程度呢？能组建舰队，把大航海时代杀过来的葡萄牙人赶回去，占领了肯尼亚蒙巴萨等一连串的印度洋港口，国土从巴基斯坦延伸到南非，现在中国努力开发的巴基斯坦瓜达尔港，当年也是阿曼帝国的海外领地。19世纪的时候，阿曼苏丹和暂时忙于列强争霸的英国一度平起平坐谈条约，为表示诚意送给英王威廉四世一艘74炮战舰！还派使节到法国和美国谈通商协定。

为了保证印度洋的贸易利润，赛义德·伊本·苏尔坦迁都到桑给巴尔，就是莫桑比克外海的岛屿。后来两个儿子一个称阿曼苏丹，一个称桑给巴尔苏丹，都凭借舰队称孤道寡，到1890年才被英国控制。在19世纪大多数时间里，大清要是和南部非洲拼海军，怕是也占不到什么便宜啊。（和北非就不要比了，埃及阿里在19世纪早期能造战列舰，质量不比欧洲国家差，敢和欧洲联军打海战）

参考：

https://www.zhihu.com/question/36045046/answer/66432486

如果2015年的整个撒哈拉以南非洲和1895年的清帝国开战，谁能赢？

----

富士山头扬汉旗,樱花树下醉胡姬。

----

4月20日，一则刊发在中国文物交流中心官网上的邀展信息引起了关注。这批“求接待”的珍贵文物来自阿富汗国家博物馆，2017年3月来到中国故宫博物院展出。按照原计划，这批宝藏在6月17日故宫展览结束后将前往美国芝加哥大学，但是芝加哥大学意外取消了展览安排，这批宝藏马上要被迫回到战火纷飞的阿富汗。

局势危急。经过重重努力，这批来自阿富汗国家博物馆的宝藏又先后在敦煌博物院、成都博物馆进行了巡展。5月6日结束在成都的巡展后，它们还将赴郑州博物馆、（深圳）南山博物馆展出至今年10月。但是，保管这批文物的要求很高，至今还没有博物馆接手下一个展览。

参考：

http://www.sohu.com/a/229009618_731021

流浪的国宝：一场阿富汗文物的“接力”

http://www.fx361.com/page/2017/0620/1923337.shtml

流浪的国宝 阿富汗珍宝背后的故事

----

Bendegó陨石被称作“镇馆之宝”，它摆在博物馆的入口处，是目前在巴西境内发现的最大铁陨石，五吨多重。

1784年，巴西东北部的巴伊阿州，一个小男孩寻找一头迷路的牛，意外发现了铁陨石。当时它是世界上已发现的第二大陨石。把它从深山里搬来里约可是一大工程，先是用了数十头牛拉车，后来车失控，陨石反而跌落到溪流中。一百多年后，一个退休的海军军官负责搬运，他的人马费劲周折，马车，铁路，船只一起上，才在1888年把它运到了现在的位置。

https://zhuanlan.zhihu.com/p/43917269

巴西国家博物馆大火，人类一手造成了多少“文化悲剧”？

https://mp.weixin.qq.com/s/9M5wHENhL2BKa4tdng4X0A

5个小时大火，人类的这段记忆被清空了……

----

![](/images/img2/FYWX.jpg)
