---
layout: post
title:  数学狂想曲（三）——随机变量序列的收敛性, 统计模拟, 隐式狄利克雷划分, 频率统计学派 vs. 贝叶斯学派
category: theory 
---

# 随机变量序列的收敛性

弱收敛：$$F_n(x)\xrightarrow{W}F(x)$$

依分布收敛：$$X_n\xrightarrow{L}X$$

依概率收敛：$$X_n\xrightarrow{P}X$$

r阶收敛：$$X_n\xrightarrow{r}X$$

几乎处处收敛（almost everywhere convergent）：$$X_n\xrightarrow{a.e.}X$$ or $$X_n\xrightarrow{a.s.}X$$

一致收敛（uniform convergence）：$$X_n\xrightarrow{u.c.}X$$

以上概念实际上都是测度论的内容。具体到这里，弱收敛针对分布函数F，而其他收敛针对随机变量X。

收敛严格性：

$$X_n\xrightarrow{P}X>X_n\xrightarrow{L}X$$

$$X_n\xrightarrow{r}X>X_n\xrightarrow{P}X$$

$$X_n\xrightarrow{a.s.}X>X_n\xrightarrow{P}X$$

大数定理：

依概率收敛->弱大数定理

几乎处处收敛->强大数定理

# 统计模拟

统计模拟是数理统计中非常有用的工具之一， 它是利用计算机产生某概率模型的随机数，再通过这些随机数来模拟真实模型。

这方面的书籍首推Sheldon M.Ross所著的《Simulation》。

>注：Sheldon M.Ross，斯坦福大学统计学博士（1968），UCB教授（1976～2004），南加州大学Industrial and Systems Engineering系主任。

以下仅记录一些简单的结论。

## 均匀分布

通过线性同余发生器可以生成伪随机数，且该随机数满足均匀分布。

$$x_n=ax_{n-1}\quad \mathrm{modulo} \quad m$$

## 正态分布

**Box-Muller变换**：如果随机变量$$U_1,U_2$$独立且$$U_1, U_2 \sim Uniform(0,1)$$，

$$\begin{align} 
Z_0 & = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ 
Z_1 & = \sqrt{-2\ln U_1} sin(2\pi U_2) 
\end{align}$$

则$$Z_0,Z_1$$独立且服从标准正态分布。

>注：George Edward Pelham Box，1919~2013，英国统计学家。伦敦学院大学博士，先后供职于普林斯顿大学和威斯康辛-麦迪逊大学。Ronald Aylmer Fisher的女婿。英国皇家学会会员，美国统计协会主席，数理统计学会（这是一个国际组织）主席。

>Mervin Edgar Muller，俄亥俄州立大学教授。

# 隐式狄利克雷划分

Latent Dirichlet Allocation，简称LDA。注意不要和Linear Discriminant Analysis搞混了。

这方面的文章，首推rickjin（靳志辉）写的《LDA数学八卦》一文。全文篇幅长达55页，我实在没有能力写的比他更好，因此这里就做一个摘要好了。

>注：靳志辉，北京大学计算机系计算语言所硕士，日本东京大学情报理工学院统计自然语言处理方向博士。2008 年加入腾讯，主要工作内容涉及统计自然语言处理和大规模并行机器学习工具的研发工作。 目前为腾讯社交与效果广告部质量研发中心总监，主要负责腾讯用户数据挖掘、精准广告定向、广告语义特征挖掘、广告转化率预估等工作。   
>他写的另一篇文章《正态分布的前世今生》，也是统计界著名的科普文，非常值得一看。

## 马氏链及其平稳分布

Markov chain的定义如下：

$$P(X_{t+1}=x|X_t, X_{t-1}, \cdots) =P(X_{t+1}=x|X_t)$$

即状态转移的概率只依赖于前一个状态的随机过程。

>注：上面是1阶Markov过程的定义。类似的还可以定义n阶Markov过程，即状态转移的概率只依赖于前n个状态的随机过程。

>Andrey(Andrei) Andreyevich Markov，1856~1922，俄国数学家。圣彼得堡大学博士，导师Pafnuty Chebyshev。最早研究随机过程的数学家之一。圣彼得堡学派的第二代领军人物，俄罗斯科学院院士。   
>虽然现在将随机过程（stochastic process）划为数理统计科学的一部分，然而在19世纪末期，相关的研究者在学术界分属两个不同的团体。其中最典型的就是英国的剑桥学派（Pearson、Fisher等）和俄国的圣彼得堡学派（Chebyshev、Markov等）。因此将Markov称为统计学家是非常错误的观点。

一些非周期的马氏链，经过若干次的状态转移之后，其状态概率会收敛到一个特定的数值，即平稳分布。

如各参考文献所示，这里一般会举社会学上的人口分层问题，引出马氏链的极限和平稳分布的概念。这里要特别注意马氏链收敛定理以及它的具体含义。

**细致平稳条件**：如果非周期马氏链的转移矩阵P和分布$$\pi(x)$$满足

$$\pi(i)P_{ij} = \pi(j)P_{ji} \quad\quad \text{for all} \quad i,j$$

则$$\pi(x)$$是马氏链的平稳分布，上式被称为细致平稳条件(detailed balance condition)。

满足细致平稳条件的马氏链，其后续状态都是平稳分布状态，不会再改变。

参考：

http://blog.csdn.net/lanchunhui/article/details/50451620

http://blog.csdn.net/pipisorry/article/details/46618991

## MCMC

对于给定的概率分布$$p(x)$$，我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布，于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是$$p(x)$$，那么我们从任何一个初始状态$$x_0$$出发沿着马氏链转移，得到一个转移序列$$x_0, x_1, x_2, \cdots x_n, x_{n+1}\cdots$$，如果马氏链在第n步已经收敛了，于是我们就得到了$$\pi(x)$$的样本$$x_n, x_{n+1}\cdots$$。

Markov Chain Monte Carlo算法的核心是引入一个参数$$\alpha(i,j)$$使得一个普通的马氏链，变成一个满足细致平稳条件的马氏链。即：

$$p(i) \underbrace{q(i,j)\alpha(i,j)}_{Q'(i,j)} 
= p(j) \underbrace{q(j,i)\alpha(j,i)}_{Q'(j,i)}  \quad$$

以上即为Metropolis算法。

>注：Nicholas Constantine Metropolis，1915~1999，希腊裔美籍物理学家。芝加哥大学博士，反复供职于Los Alamos和芝加哥大学。（其实也就这俩地方，只不过这边干几年到那边去，那边教几年书再回这边来，这么进进出出好几个来回而已）“曼哈顿计划”的主要科学家之一，战后主持MANIAC计算机的研制。

$$\alpha$$的大小，决定了马氏链的收敛速度。$$\alpha$$越大，收敛越快。因此又有Metropolis–Hastings算法，其关键公式为：

$$\alpha(i,j) = \min\left\{\frac{p(j)q(j,i)}{p(i)q(i,j)},1\right\}$$

>注：Wilfred Keith Hastings，1930~2016，美国统计学家，多伦多大学博士，维多利亚大学教授。

## Gibbs Sampling

>注：Josiah Willard Gibbs，1839~1903，美国科学家。他在物理、化学和数学方面都有重大理论贡献。耶鲁大学博士和教授。统计力学的创始人。

因为高维空间中，沿坐标轴方向上的两点之间的转移，满足细致平稳条件。因此，Gibbs Sampling的核心就是沿坐标轴循环采样，该算法收敛之后的采样点即符合指定概率分布。

## Unigram Model

![](/images/article/unigram-model.jpg)

![](/images/article/dirichlet-multinomial-unigram.jpg)

$$Dir(\overrightarrow{p}|\overrightarrow{\alpha})+MultCount(\overrightarrow{n})=Dir(\overrightarrow{p}|\overrightarrow{\alpha}+\overrightarrow{n})$$

## PLSA

Probabilistic Latent Semantic Analysis

![](/images/article/plsa-doc-topic-word.jpg)

## LDA

![](/images/article/lda-dice.jpg)

# 频率统计学派 vs. 贝叶斯学派

对数学史感兴趣的朋友，可以看看陈希孺院士的《数理统计学简史》一书。rickjin文章的内容有相当部分取自该书。

>注：陈希孺，1934～2005，数理统计学家。1956年毕业于武汉大学数学系，1997年当选为中国科学院院士。

该书中关于频率统计学派和贝叶斯学派的争议，引起了我的注意。

频率统计学派是所谓的正统派，由于其简单且便于理解的特点，多数入门级的数理统计学教程，一般都是按照该学派的思路写的。

而贝叶斯学派可谓另辟蹊径，它和频率统计学派的差异，参见《机器学习（七）》。由于该派系的思想比较新颖，我一度以为它和频率统计学派的关系，就犹如相对论之于经典力学。

然而，陈希孺院士告诉我们，两者各有优劣，尚未到一方决出胜负的阶段。比如，贝叶斯学派的先验估计，既是其成功的奥秘，也是其不成功的软肋。比如，对于“无信息先验分布”，目前尚处于“信则灵，不信则无”的境地。

陈院士的观点是：各取所长，为我所用。


