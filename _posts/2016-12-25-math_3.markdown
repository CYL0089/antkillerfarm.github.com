---
layout: post
title:  数学狂想曲（三）——统计模拟
category: theory 
---

# 统计模拟

统计模拟是数理统计中非常有用的工具之一， 它是利用计算机产生某概率模型的随机数，再通过这些随机数来模拟真实模型。

这方面的书籍首推Sheldon M.Ross所著的《Simulation》。

>注：Sheldon M.Ross，斯坦福大学统计学博士（1968），UCB教授（1976～2004），南加州大学Industrial and Systems Engineering系主任。

以下仅记录一些简单的结论。

## 均匀分布

通过线性同余发生器可以生成伪随机数，且该随机数满足均匀分布。

$$x_n=ax_{n-1}\quad \mathrm{modulo} \quad m$$

## 正态分布

**Box-Muller变换**：如果随机变量$$U_1,U_2$$独立且$$U_1, U_2 \sim Uniform(0,1)$$，

$$\begin{align} 
Z_0 & = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ 
Z_1 & = \sqrt{-2\ln U_1} sin(2\pi U_2) 
\end{align}$$

则$$Z_0,Z_1$$独立且服从标准正态分布。

>注：George Edward Pelham Box，1919~2013，英国统计学家。伦敦学院大学博士，先后供职于普林斯顿大学和威斯康辛-麦迪逊大学。Ronald Aylmer Fisher的女婿。英国皇家学会会员，美国统计协会主席，数理统计学会（这是一个国际组织）主席。

>Mervin Edgar Muller，俄亥俄州立大学教授。

# 神经网络

这部分最主要的参考文献包括：

《机器学习》，周志华著。

《Deep Learning Tutorial》，李宏毅著（台湾大学电机工程学助理教授）。

http://www.useit.com.cn/thread-13132-1-1.html

其他参考文献将在相关部分列出。

## MP神经元模型

MP神经元模型是1943年，由Warren McCulloch和Walter Pitts提出的。

>注：Warren Sturgis McCulloch，1898~1969，美国神经生理学和控制论科学家。哥伦比亚大学博士，先后执教于MIT、Yale、芝加哥大学。

>Walter Harry Pitts, Jr.，1923~1969，美国计算神经学科学家。   
>这个人的经历，实在是非典型。家里贫穷，大约是读不起大学，15岁的时候，到芝加哥大学旁听Bertrand Russell的讲座。Russell很看重这个年轻人，但由于他只是访问学者，于是在回国之前，将Pitts介绍给Rudolf Carnap，后者为Pitts安排了一份在学校打杂的工作。这一打杂就是五六年时间，最后凭借论文，获得芝加哥大学的准学士学位（因为他始终都不是正式学籍的学生），这也是他一生唯一的学位。   
>但是如果看看Pitts的合作者的阵容，就知道Pitts水平之高了。他们是：Warren McCulloch、Jerome Lettvin、Norbert Wiener。

MP神经元模型如下图所示：

![](/images/article/MP_model.png)

即：

$$y_j=\sum _{i=1}^nw_{ij}x_i-\theta_j$$

上式其实就是《机器学习（一）》中提到的逻辑回归。

除了阶跃函数和Sigmod函数之外，常用的神经元激活函数，还有双曲正切函数：

$$f(z)=\tanh(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$

其导数为：

$$f'(z)=1−(f(z))^2$$

生物神经元和MP神经元模型的对应关系如下表：

| 生物神经元 | MP神经元模型 |
|:--:|:--:|
| 神经元 | $$j$$ |
| 输入信号 | $$x_i$$ |
| 权值 | $$w_{ij}$$ |
| 输出信号 | $$y_j$$ |
| 总和 | $$\sum$$ |
| 膜电位 | $$\sum _{i=1}^nw_{ij}x_i$$ |
| 阈值 | $$\theta_j$$ |

从上图亦可看出，如果将阈值看作输入为-1.0的哑节点的连接权重，则权重和阈值可统一为权重。神经网络训练的过程，实际上就是根据样本调整权重和阈值的过程。

参考：

http://blog.csdn.net/u013007900/article/details/50066315

## 单层感知器 vs. 多层感知器

神经网络的层数越多，其表达力越丰富，如下表所示：

![](/images/article/single_layer_vs_multi_layer.png)

## ANN简史

![](/images/article/ANN_history.png)

## BP算法

误差逆传播（error BackPropagation）算法最早由Paul J. Werbos于1974年提出，然而此时正值ANN的低谷，未得到人们的重视。因此到了1986年时，由David Everett Rumelhart重新发明了该算法。

>注：Paul J. Werbos，1947年生，哈佛大学博士。

>David Everett Rumelhart，1942~2011，美国心理学家。斯坦福大学博士，先后执教于UCSD和斯坦福。美国科学院院士。

![](/images/article/BP_network.png)

BP算法的核心思路：

1.利用前向传导公式，计算第n层输出值。

2.计算输出值和实际值的残差。

3.将残差梯度传递回第$$n-1,n-2,\dots,2$$层，并修正各层参数。（即所谓的误差逆传播）

BP算法的推导过程教材已经写的很好了，这里只补充一点：

神经网络的参数的随机初始化的目的是使对称失效。否则的话，所有对称结点的权重都一致，也就无法区分并学习了。

# PID算法

PID算法是自动控制领域最基础应用也最广泛的算法。它是三个单词proportional（P，比例）、integral（I，积分）和differential（D，微分）的缩写。

PID算法的流程如下图所示：

| 图1 |
|:--:|
| ![](/images/article/PID.png) |

以下我们以水箱水龙头为例，解释一下PID算法的各个要素。

**场景1**：假设我们面对的系统是一个简单的水箱的液位，要从空箱开始注水直到达到某个高度，而你能控制的变量是注水龙头的开关大小。

图1中的$$r(t)$$表示期待的设定值，在这个场景中，就是水箱的液位。$$y(t)$$表示当前的液位。$$e(t)=r(t)-y(t)$$表示误差值。

针对这个场景，我们可以设计如下算法：

水箱液位离预定高度远的时候开关开大点，离的近的时候开关就开小点，随着液位逐步接近预定高度逐渐关掉水龙头。用数学表示就是：

$$u(t) = K_\text{p} e(t)\tag{1}$$

上式中的$$u(t)$$表示需要控制的量，在这里就是水龙头的开合度。$$K_\text{p}$$被称为比例系数。

**场景2**：假设这个水箱不仅仅是装水的容器了，还需要持续稳定的给用户供水。

以下用c表示给用户供水的量($$c\ge 0$$)。显然如果使用公式1，则系统稳定时，$$u(t)-c=0$$，即$$K_\text{p} e(t)=c$$。

由于c和$$K_\text{p}$$都不为0，因此$$e(t)$$也不为0，这就导致始终无法加注到指定水位。这种稳态误差被称为静差。

为了平衡c，我们修改算法为：

$$u(t) = K_\text{p} e(t) + K_\text{i} \int_0^t e(\tau) \,d\tau\tag{2}$$

$$K_\text{i}$$被称为积分系数。积分环节的意义就相当于你增加了一个水龙头，这个水龙头的开关规则是水位比预定高度低就一直往大了拧，比预定高度高就往小了拧。如果漏水速度不变，那么总有一天这个水龙头出水的速度恰好跟漏水的速度相等了，系统就和第一小节的那个一样了。那时，静差就没有了。这就是所谓的积分环节可以消除系统静差。

**场景3**：假设用户的用水量是变化的，即$$c(t)$$。

这时，如果仍采用公式2，则由于$$c(t)$$的变化，导致$$e(t)$$不恒为0。为了减小$$c(t)$$的变化，对$$e(t)$$的影响，我们继续修改算法：

$$u(t) = K_\text{p} e(t) + K_\text{i} \int_0^t e(\tau) \,d\tau + K_\text{d} \frac{de(t)}{dt}\tag{3}$$

$$K_\text{d}$$被称为微分系数。由于$$c(t)$$的变化不可能事先得知，因此，微分环节只能减小$$c(t)$$的变化所造成的影响，而不能消除。

公式3在Laplace域可写做：

$$L(s) = K_\text{p} + K_\text{i}/s + K_\text{d} s\tag{4}$$

从公式4可以看出，PID controller的数学原理和锁相环（Phase-locked loop）非常类似，它们实际上都是Feedback Control系统。

![](/images/article/PLL.png)

参考：

https://en.wikipedia.org/wiki/PID_controller

《Feedback Control of Dynamic Systems》，Gene F. Franklin，J David Powell，Abbas Emami-Naeini著。

>注：Gene F. Franklin，1927~2012，美国控制论学家。哥伦比亚大学博士，斯坦福大学教授。

>J David Powell，美国航空航天学家。斯坦福大学博士和教授。

>Abbas Emami-Naeini，斯坦福大学博士和讲师，SC Solutions公司总监。

https://www.zhihu.com/question/23088613/answer/23942834

http://blog.163.com/suyanqiang0613@126/blog/static/1100531332012111475650867/

# GBDT

GBDT这个算法有很多名字，但都是同一个算法：

GBRT (Gradient BoostRegression Tree)渐进梯度回归树

GBDT (Gradient BoostDecision Tree)渐进梯度决策树

MART (Multiple Additive Regression Tree)多决策回归树

Tree Net决策树网络

GBDT属于集成学习（Ensemble Learning）的范畴。集成学习的思路是在对新的实例进行分类的时候，把若干个单个分类器集成起来，通过对多个分类器的分类结果进行某种组合来决定最终的分类，以取得比单个分类器更好的性能。

集成学习的算法主要分为两大类：

**并行算法**：若干个不同的分类器同时分类，选择票数多的分类结果。这类算法包括bagging和随机森林等。

**串行算法**：使用同种或不同的分类器，不断迭代。每次迭代的目标是缩小残差或者提高预测错误项的权重。这类算法包括Adaboost和GBDT等。

GBDT写的比较好的，有以下blog：

http://blog.csdn.net/w28971023/article/details/8240756

摘录要点如下：

决策树分为两大类：

**回归树**：用于预测实数值，如明天的温度、用户的年龄、网页的相关程度。其结果加减是有意义的，如10岁+5岁-3岁=12岁。

**分类树**：用于分类标签值，如晴天/阴天/雾/雨、用户性别、网页是否是垃圾页面。其结果加减无意义，如男+男+女=到底是男是女？

GBDT的核心在于**累加所有树的结果作为最终结果**。例如根到某叶子结点的路径上的决策值为10岁、5岁、-3岁，则该叶子的最终结果为10岁+5岁-3岁=12岁。

所以**GBDT中的树都是回归树，不是分类树**。

上面举的例子中，越靠近叶子，其决策值的绝对值越小。这不是偶然的。决策树的基本思路就是“分而治之”，自然越靠近根结点，其划分的粒度越粗。每划分一次，预测误差（即残差）越小，这样也就变相提高了下一步划分中预测错误项的权重，这就是算法名称中Gradient的由来。

为了防止过拟合，GBDT还采用了Shrinkage（缩减）的思想，每次只走一小步来逐渐逼近结果，这样各个树的残差就是渐变的，而不是陡变的。

参考：

http://blog.csdn.net/u010691898/article/details/38292937

http://www.cs.cmu.edu/~tom/

# 20世纪10大算法

2000年，IEEE评选出20世纪10大算法。名单如下：

1.[Metropolis Algorithm for Monte Carlo](http://en.wikipedia.org/wiki/Metropolis_Algorithm)

2.[Simplex Method for Linear Programming](http://en.wikipedia.org/wiki/Simplex_Method)

3.[Krylov Subspace Iteration Methods](http://en.wikipedia.org/wiki/Krylov_subspace_methods)

4.[The Decompositional Approach to Matrix Computations](http://en.wikipedia.org/wiki/Matrix_(mathematics)#Decomposition)

5.[The Fortran Optimizing Compiler](http://en.wikipedia.org/wiki/Fortran#History)

6.[QR Algorithm for Computing Eigenvalues](http://en.wikipedia.org/wiki/QR_algorithm)

7.[Quicksort Algorithm for Sorting](http://en.wikipedia.org/wiki/Quicksort)

8.[Fast Fourier Transform](http://en.wikipedia.org/wiki/Fast_Fourier_Transform)

9.[Integer Relation Detection](http://en.wikipedia.org/wiki/Integer_relation_algorithm)

10.[Fast Multipole Method](http://en.wikipedia.org/wiki/Fast_Multipole_Method)

详细内容参见：

http://www.uta.edu/faculty/rcli/TopTen/topten.pdf

中文版本：

http://blog.csdn.net/v_JULY_v/article/details/6127953


