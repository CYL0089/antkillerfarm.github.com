---
layout: post
title:  Hadoop, Spark, Kettle
category: technology 
---

# Hadoop

最近（2016.4），参加公司组织的内部培训，对Hadoop有了一些认识，特记录如下。

## 概述

Hadoop项目由Doug Cutting创建。Doug Cutting也是Lucene项目的创建者。

PS：Lucene是我2007年学习搜索引擎技术时，所接触到的开源项目。回想起来，简直恍如隔世啊。

官网：

http://hadoop.apache.org/

官方文档：

http://hadoop.apache.org/docs/current/

Hadoop框架及生态圈：

http://cqwjfck.blog.chinaunix.net/uid-22312037-id-3969789.html

www.360doc.com/content/14/0113/17/15109633_345010019.shtml

## 编译

Hadoop目前不在Ubuntu的官方软件仓库中，无法使用apt-get安装。使用源代码编译Hadoop的相关步骤，可在源码包的BUILDING.txt中找到。这里仅作为补遗之用。总的来说，如无必要还是直接下载bin包比较好，编译还是很麻烦的。

### 安装FindBugs

Hadoop的大部分软件依赖，都可以使用apt-get安装。BUILDING.txt里已经写的很详细了。

FindBugs是一个Java代码的静态分析检查工具。它的官网：

http://findbugs.sourceforge.net/index.html

它的安装方法有3种：

1.源代码安装。下载源代码之后，运行`ant build`，然后设定相关路径，以供Hadoop使用。

2.apt-get安装。FindBugs目前不在Ubuntu 14.04的软件仓库中，而在Ubuntu 15.10的软件仓库中，需要设置源方可安装。这种方法也需要设定相关路径，以供Hadoop使用。

2.maven安装。

`mvn compile findbugs:findbugs`

这种方法最简单。安装完成之后的FindBugs位于maven repository中，而后者通常在~/.m2/repository/下。

这种方法的好处是：由于Hadoop使用maven编译，maven安装之后，可以免去设置路径的步骤。但坏处是：其他不用maven的程序，无法使用该软件。

这一步只要不出下载不成功之类的错误，就算成功。错误留给下一步来解决。

PS：maven下载的文件，大约有180MB，且多为小文件，初次运行相当费时。

### 编译Hadoop

`mvn package -Pdist -DskipTests -Dtar`

这里一定要`-DskipTests`，原因是test不仅速度非常慢，会导致系统响应缓慢，而且即使是官方代码，也不一定能通过所有的test case。

编译的结果在hadoop-dist/target下

## 安装

Hadoop有Single Node和Cluster两种安装模式。一般的部署，当然采用后者。得益于Java的可移植性，Hadoop甚至可以部署到由Raspberry Pi组成的集群中。

前者主要用于开发和学习之用。这里只讨论前者。

Single Node又可分为两种模式：Standalone和Pseudo-Distributed。前者一般仅用于检验程序逻辑的正确性，因为这种模式下，并没有Hadoop常见的各种节点和HFS的概念，所有的程序都运行在一个Java进程中。而后者在配置和运行方面，与Cluster已经相差无几。

http://www.cnblogs.com/serendipity/archive/2011/08/23/2151031.html

## 何时使用hadoop fs、hadoop dfs与hdfs dfs命令

hadoop fs：使用面最广，可以操作任何文件系统。

hadoop dfs与hdfs dfs：只能操作HDFS文件系统相关（包括与Local FS间的操作），前者已经Deprecated，一般使用后者。

# Hbase

http://hbase.apache.org/

`sbin/start-dfs.sh`

`bin/start-hbase.sh`

http://www.cnblogs.com/cenyuhai/

这个blog专注于hadoop、hbase、spark。

## 基本概念

HBase以表的形式存储数据。表有行和列组成。列划分为若干个列族/列簇(column family)。

<table>
<tbody>
<tr border="1">
<td rowspan="2"><strong>Row Key</strong></td>
<td colspan="2"><strong>column-family1</strong></td>
<td colspan="3"><strong>column-family2</strong></td>
<td><strong>column-family3</strong></td></tr>
<tr>
<td><strong>column1</strong></td>
<td><strong>column2</strong></td>
<td><strong>column1</strong></td>
<td><strong>column2</strong></td>
<td><strong>column3</strong></td>
<td><strong>column1</strong></td></tr>
<tr>
<td><strong>key1</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><strong>key2</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><strong>key3</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr></tbody></table>

如上图所示，key1,key2,key3是三条记录的唯一的row key值，column-family1,column-family2,column-family3是三个列族，每个列族下又包括几列。比如，column-family1这个列族下包括两列，名字是column1和column2。

## 常用的HBase Shell命令

名称 | 命令表达式
创建表 | create '表名称', '列名称1','列名称2','列名称N'
添加记录 | put '表名称', '行名称', '列名称:', '值'
查看记录 | get '表名称', '行名称'
查看表中的记录总数 | count  '表名称'
删除记录 | delete  '表名' ,'行名称' , '列名称'
删除一张表 | 先要屏蔽该表，才能对该表进行删除，第一步 disable '表名称' 第二步  drop '表名称'
查看所有记录 | scan "表名称"  
查看某个表某个列中所有数据 | scan "表名称" , ['列名称:']
更新记录 | 就是重写一遍进行覆盖

# Spark

http://spark.apache.org/

## RDD

Resilient Distributed Datasets是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。RDD克服了传统的MapReduce所进行大量的磁盘IO操作。

我的理解：RDD将计算包括中间结果，全部放到内存中，以节省磁盘IO操作。

http://www.infoq.com/cn/articles/spark-core-rdd

http://f.dataguru.cn/thread-475874-1-1.html

## DataFrame和DataSet

http://www.jianshu.com/p/c0181667daa0

http://www.csdn.net/article/2015-02-17/2823997

https://www.iteblog.com/pdf/1675

## Spark部署

Spark没有服务程序，因此无须部署，只需要在集群中的某台机器上安装spark，进行任务提交即可。

参见：

http://blog.csdn.net/book_mmicky/article/details/25714287

Spark虽然对Hadoop的版本有一定的要求，但是并不是太严重的问题。比如，目前最新的Spark 2.0.1（2016.10）仍然支持Hadoop 2.3，而后者是2014年2月出的版本。

## Spark vs Hadoop

http://www.bkjia.com/Linux/1076682.html

https://www.dezyre.com/article/hadoop-mapreduce-vs-apache-spark-who-wins-the-battle/83

## 基本统计操作

1.统计列中不相同值的个数。

方法一：

比如在观众观影的表格中，找出观众的数量或电影的数量。

`Dataset<Row> df = session.sql("SELECT userId FROM Movie group by userId");//df.count() is the count of different values`

方法二：

Spark中有个org.apache.spark.sql.functions类，专门针对数据集进行各种运算操作。其中的countDistinct方法可实现该功能。片段示例如下：

`Dataset<Row> df2 = df.agg(functions.countDistinct("userId"));df2.show();`

# Parquet

Parquet是Twitter开源的一种供Hadoop使用的列式存储格式。

参考：

http://blog.csdn.net/dc_726/article/details/41777661

从NSM到Parquet：存储结构的衍化

http://blog.csdn.net/dc_726/article/details/41143175

几张图看懂列式存储

# Kettle

ETL，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。这个过程有时又被叫做“数据清洗”。

Kettle是一款国外开源的ETL工具。其官网为：

http://community.pentaho.com/projects/data-integration/

从大的方面来说，Kettle的工作，可分为Job和Trans两类。

Job是个大尺度的工作分类，它操作的对象是文件或数据库级别的。

相对的，Trans只要是些小尺度的工作。它操作的对象是Row，也就是一条记录。

Sqoop是另一个数据转换工具，但是没有IDE，功能也局限于数据格式的转换（仅相当于Kettle中的Trans）。其官网：

http://sqoop.apache.org/

Job可以调用Trans，也可以执行shell、SQL、Sqoop等多种脚本。因此，Kettle和Sqoop的关系不是二选一，而是可以协作的。

Kettle是一个大的工具集，主要包括以下组件：

1.spoon。Kettle的IDE。

2.pan。用于执行Trans文件（以ktr为后缀）。

3.Kitchen。用于执行Job文件（以kjb为后缀）。

## 命令行执行Job和Trans

`pan -file=/path/to/ktr`

`kitchen -file=/path/to/kjb`

参见：

http://www.cnblogs.com/wxjnew/p/3620792.html

官网资料：

http://wiki.pentaho.com/display/EAI/Pan+User+Documentation

http://wiki.pentaho.com/display/EAI/Kitchen+User+Documentation

命令行不仅可以执行Job和Trans，还可以向Job和Trans传递参数。参见pan和kitchen命令的-param选项。

Job和Trans对命令行参数的处理，有相关的插件，参见：

http://blog.csdn.net/scorpio3k/article/details/7872179

## 文本处理

Kettle的文本处理以“行”为单位。下图是一个实际的使用流程图：

![](/images/article/kettle.png)

1.原始数据来自网络爬虫抓取的数据，它的主体是一个json文件，然而在每一条记录的前后都有一些特殊的字符串，因此从整体来说，并不是一个合法的json文件。

2.采用“字符串替换”插件，去除非法字符串。这里需要注意的是，由于整个过程是数据流形式的，因此，无法在一个步骤中，同时去掉前后两个字符串，而必须分为两个步骤。

3.替换之后，原先有字符串的行，可能变成空行，这是可以使用“过滤记录”插件。

## 文件的增量处理

![](/images/article/kettle_2.png)

kettle没有提供直接的插件用于增量处理，因此需要自己设计增量处理的方法。

增量处理的方法很多，这里仅展示其中一种方法：

1.获得需要处理文件总表A。

2.获得已经处理过的文件列表B。这个列表可以来源于数据库，也可来源于文本文件。这里采用后者。

3.使用“合并记录”插件，从A中过滤掉B。“合并记录”插件的flagfield字段，会给出合并的结果。

4.使用“过滤记录”插件，根据flagfield字段的结果，得到过滤后的列表C。C就是真正需要处理的文件列表了。

## HDFS

HDFS的处理比较简单，将普通例子中的本地文件路径，替换为hdfs://形式的hdfs路径即可。

## Hbase

Hbase插件中的其他概念都比较好理解。唯一比较费解的是Mapping name这个名词，实际上它就是Hbase中表的列族/列簇(column family)。

# Storm

Storm是一个大数据领域的实时计算框架。

官网：

http://storm.apache.org/

教程：

http://storm.apache.org/releases/current/Tutorial.html

## Storm vs Spark

http://blog.csdn.net/iefreer/article/details/32715153


