---
layout: post
title:  数学狂想曲（六）——自相关&互相关&卷积, 闵可夫斯基距离, 马氏距离, 数学的深渊, Gabriel's Horn, 函数连续性
category: math 
---

# 概率分布（2）

## KS检验（续）

>Vladimir Andreevich Steklov，1864～1926，俄国数学家、物理学家。哈尔科夫大学博士，其导师是圣彼得堡学派第二代人物中，仅次于Andrey Markov的Aleksandr Lyapunov。哈尔科夫大学和圣彼得堡大学教授，1919年创建斯塔克罗夫数学研究所。

>斯塔克罗夫数学研究所是一家专职研究没有教学任务和科研任务的研究机构。Grigori Perelman在这里，曾有6年时间没有发表一篇论文。二十世纪俄罗斯绝大多数的数学发现都源自这里。

![](/images/article/KS.png)

上图的红线是某随机变量假设分布的CDF，而蓝线是该随机变量样本的累积分布曲线，即ECDF（Empirical Distribution Function）。

显然若假设正确的话，两条曲线应该是基本重合的。反之，若两条曲线差异较大，则该假设检验不成立。这就是KS检验的基本原理。

KS检验的统计量定义如下：

$$D_n= \sup_x |F_n(x)-F(x)|$$

其中$$\sup$$表示最小上界，

$$F_n(x)={1 \over n}\sum_{i=1}^n I_{[-\infty,x]}(X_i)$$

$$I_{[-\infty,x]}(X_i)=\begin{cases}
1, & X_i \le x \\
0, & \text{otherwise} \\
\end{cases}$$

KS检验更深入的解释，涉及到布朗运动和维纳过程，这里不再赘述。

# 自相关&互相关&卷积

![](/images/article/Comparison_convolution_correlation.svg)

1.**自相关（Autocorrelation）**。这个最简单，就是平移之后，自己和自己比。显然当平移为0的时候，自相关值最大，因此这类操作通常用于信号的检测。信号接收端模拟发射端的信号序列，对实际接收到的信号进行相关操作，只有当两者的序列接近重合时，才会检测到信号峰值。

2.**互相关（Cross-correlation）**。检测两个序列的相似度，显然两者越相似，互相关值越大。这在统计学方面用的比较多。

在信号处理领域，Cross-correlation常用于信号的调制和解调。下图就是常见的正交调制解调（orthogonal modulation and demodulation）的原理图：

![](/images/article/iq_modulator.png)

![](/images/article/iq_demodulator.png)

3.**卷积（Convolution）**。卷积主要用于线性时不变系统的信号处理。相比于互相关操作，卷积有个旋转180度的操作，这里解释一下它的物理意义。

例如，当一个拳击选手遭到对方连续两次击打身体的同一部位时，第二次被击打时他感觉到的疼痛是第一次被击打所遗留的疼痛与第二次被击打的疼痛之和。即：

$$f(2)=f_1(2)+f_2(1)$$

其中，$$f_i(t)$$中，i表示第i次击打，t表示击打发生之后经过的时间。可以看出i和t的顺序正好是相反的，这也就是Convolution这个名词的本意。这里假设g为常数。

4.这三个操作在离散域最终都可以变为求和操作，也就是向量内积运算。我们一般使用$$a\cdot b$$或者$$\langle a,b\rangle$$表示向量的内积运算。即：

$$\langle a,b\rangle=a_0b_0+a_1b_1+\dots+a_nb_n$$

卷积最经典的应用，当属信号处理领域的傅里叶变换：

$$\mathcal{F}\{f * g\} = k\cdot \mathcal{F}\{f\}\cdot \mathcal{F}\{g\}$$

# 闵可夫斯基距离

>Hermann Minkowski（1864-1909），德国数学家，哥廷根大学数学教授，爱因斯坦的老师。

Minkowski distance的定义：

$$d(x,y)=\sqrt[\lambda]{\sum_{i=1}^{n}\lvert x_i-y_i\lvert^{\lambda}}$$

显然，当$$\lambda=2$$时，该距离为欧氏距离。当$$\lambda=1$$时，也被称为CityBlock Distance或Manhattan Distance（曼哈顿距离）。

# 马氏距离

Mahalanobis Distance是印度现代统计学之父Prasanta Chandra Mahalanobis于1936年提出的概念。

>注：Prasanta Chandra Mahalanobis，1893~1972，印度统计学家，剑桥大学博士，印度统计研究所创始人。   
>印度的重点研究所一般叫做Institute of National Importance，共92所。Indian Statistical Institute是其中唯一一所和统计相关的研究所。教师255，学生375，这得是多精英的教育啊。其计算机科学专业排名印度第2。

p维空间的两点（两个p维向量x,y）的欧氏距离定义为：

$$d_E(x,y)=\sqrt{(x_1-y_1)^2+\dots+(x_p-y_p)^2}=\sqrt{(x-y)^T(x-y)}（公式1）$$

因此，x到原点的距离为：

$$\parallel x\parallel=d_E(x,0)=\sqrt{(x_1)^2+\dots+(x_p)^2}（公式2）$$

也就是：

$$x_1^2+\dots+x_p^2=\parallel x\parallel^2（公式3）$$

这实际上是个正球体的方程，也就是说观测数据x的各个分量对x至中心的欧氏距离贡献是相等的。然而在统计学中我们希望寻求这样一种距离，它的各个分量的作用程度是不同的。差别较大的分量应该接受较小的权重。

于是，公式3可变形为椭球体方程：

$$(\frac{x_1}{s_1})^2+\dots+(\frac{x_p}{s_p})^2=\parallel x\parallel^2（公式4）$$

其中的$$s_i$$表示i分量的权重。

公式4进一步整理，并扩展到两个p维向量x,y，可得马氏距离定义：

$$d_M(x,y)=\sqrt{(\frac{x_1-y_1}{s_1})^2+\dots+(\frac{x_p-y_p}{s_p})^2}=\sqrt{(x-y)^TD^{-1}(x-y)}（公式5）$$

其中，$$D=diag(s_1^2,\dots,s_p^2)$$。

>注意：这里p维向量是正交基，否则的话，D将不是主对角线矩阵，而是一个普通的协方差矩阵。显然如果D为单位矩阵的话，马氏距离就变成了欧氏距离。

马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关。比如体重和身高数据，如果采用欧氏距离，则会由于量纲的不同，而无法比较。

由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。它的缺点是夸大了变化微小的变量的作用。

马氏距离的特点还包括：

1）马氏距离的计算是建立在总体样本的基础上的，这一点可以从上述协方差矩阵的解释中可以得出，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；

2）在计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离来代替马氏距离，也可以理解为，如果样本数小于样本的维数，这种情况下求其中两个样本的距离，采用欧式距离计算即可。

3）还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如A（3，4），B（5，6）；C（7，8），这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。

4）在实际应用中“总体样本数大于样本的维数”这个条件是很容易满足的，而所有样本点出现3）中所描述的情况是很少出现的，所以在绝大多数情况下，马氏距离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。

# 数学的深渊

![](/images/article/math_trench.jpg)

当然，类似的还有个AI的深渊：

![](/images/article/AI_hell.jpg)

# Gabriel's Horn

Gabriel's Horn，又名托里拆利小号（Torricelli's trumpet），是数学史上非常经典的模型。它最早由Evangelista Torricelli提出，故名。

>注：Evangelista Torricelli，1608~1647，意大利物理学家和数学家。主要研究气压、水压，提出了托里拆利原理，并发明了气压计。

![](/images/article/GabrielHorn.png)

上图是Gabriel's Horn的图像，它由$$y=\frac{1}{x}(x>1)$$绕x轴旋转得到。Torricelli发现Gabriel's Horn是个**体积有限，而表面积无限**的形状。

$$V=\pi\int\limits_1^a\left(\frac{1}{x}\right)^2\mathrm{d}x=\pi\left(1-\frac{1}{a}\right)$$

$$\lim_{a\to\infty}V=\lim_{a\to\infty}\pi\left(1-\frac{1}{a}\right)=\pi\cdot\lim_{a\to\infty}\left(1-\frac{1}{a}\right)=\pi$$

$$A=2\pi\int\limits_1^a {\frac{1}{x}}\sqrt{1+\left(-\frac{1}{x^2}\right)^2}\mathrm{d}x > 2\pi\int\limits_1^a \frac{\mathrm{d}x}{x}=2\pi\ln(a)$$

$$\lim_{a\to\infty}A\ge\lim_{a\to\infty}2\pi\ln(a)=\infty$$

但是反过来的情况，即表面积有限，而体积无限的情况，也就是所谓的Gabriel's horn逆现象，是不存在的。——实际上，众所周知的是表面积一定的形状中，球形体积最大。

Gabriel's horn现象不仅存在于表面积/体积中，也存在于长度/面积中，即所谓的无限长海岸线问题。对此的进一步研究，开拓了数学中的分形几何领域。

# 函数连续性

函数连续性本质上是刻画函数在自变量变化的情况下，函数值变化的稳定程度。

## 连续函数（Continuous function）

$$\forall x \in I \; \forall \epsilon > 0 \; \exists \delta > 0 \; \forall y \in I ;  \, |x - y|<\delta \, \Rightarrow \, |f(x) - f(y)| < \epsilon$$

连续函数有很多种等价定义，为了方便和之后的一致连续进行对比，这里选用量词（Quantifier）逻辑的定义。

这个定义实际上是个Nest定义。以$$\exists$$为界，整个Quantifier被分成3部分：定值、存在变量、变量。上边的例子实际上是说：在给定$$x, \epsilon$$的情况下，存在$$\delta$$，对所有y，结论都成立。

因为x不变，所以这实际上是个定点连续的条件。因此，这种连续又叫做Pointwise continuity。

通俗的说法：**f的函数曲线在区间内没有断点。**

## 一致连续（Uniform continuity）

$$\forall \epsilon > 0 \; \exists \delta > 0 \; \forall x, y \in I ; \, |x - y|<\delta \, \Rightarrow \, |f(x)-f(y)|<\epsilon$$

Uniform continuity比Pointwise continuity要严格一些，因为x,y都是变量。

Uniform continuity的形象解释如下所示：

![](/images/article/Continuity_and_uniform_continuity_2.gif)

上图中蓝框和红框的顶部和底部用粗线表示，在方框的运动过程中，红线**始终没有穿过方框的顶部或底部**，所以它是Uniform continuity的，而蓝线不是。

$$f(x)=\frac{1}{x}$$不是Uniform continuity的最大原因在于$$x\to 0,f(x)\to \infty$$，由于值域没有上界，因此，对于给定的$$\delta$$，$$\epsilon$$也是没有上界的。

但如果定义域限定为闭区间，则值域必有界。这时，Uniform continuity和Pointwise continuity是等价的。

## 绝对连续（Absolute continuity）

$$\forall \varepsilon > 0 \; \exists \delta > 0 \; \forall (x_k, y_k) \in I ; \, \sum_k (y_k - x_k) < \delta \, \Rightarrow \, \sum_k | f(y_k) - f(x_k) | < \varepsilon$$

Absolute continuity不仅要求整个函数满足Uniform continuity，而且任意子区间也要满足该条件。因此，Absolute continuity比Uniform continuity更严格。


