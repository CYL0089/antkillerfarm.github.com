---
layout: post
title:  数学狂想曲（六）——马氏距离, 自相关&互相关&卷积, NLP
category: theory 
---

# 马氏距离

Mahalanobis Distance是印度现代统计学之父Prasanta Chandra Mahalanobis于1936年提出的概念。

>注：Prasanta Chandra Mahalanobis，1893~1972，印度统计学家，剑桥大学博士，印度统计研究所创始人。   
>印度的重点研究所一般叫做Institute of National Importance，共92所。Indian Statistical Institute是其中唯一一所和统计相关的研究所。教师255，学生375，这得是多精英的教育啊。其计算机科学专业排名印度第2。

p维空间的两点（两个p维向量x,y）的欧氏距离定义为：

$$d_E(x,y)=\sqrt{(x_1-y_1)^2+\dots+(x_p-y_p)^2}=\sqrt{(x-y)^T(x-y)}（公式1）$$

因此，x到原点的距离为：

$$\parallel x\parallel=d_E(x,0)=\sqrt{(x_1)^2+\dots+(x_p)^2}（公式2）$$

也就是：

$$x_1^2+\dots+x_p^2=\parallel x\parallel^2（公式3）$$

这实际上是个正球体的方程，也就是说观测数据x的各个分量对x至中心的欧氏距离贡献是相等的。然而在统计学中我们希望寻求这样一种距离，它的各个分量的作用程度是不同的。差别较大的分量应该接受较小的权重。

于是，公式3可变形为椭球体方程：

$$(\frac{x_1}{s_1})^2+\dots+(\frac{x_p}{s_p})^2=\parallel x\parallel^2（公式4）$$

其中的$$s_i$$表示i分量的权重。

公式4进一步整理，并扩展到两个p维向量x,y，可得马氏距离定义：

$$d_M(x,y)=\sqrt{(\frac{x_1-y_1}{s_1})^2+\dots+(\frac{x_p-y_p}{s_p})^2}=\sqrt{(x-y)^TD^{-1}(x-y)}（公式5）$$

其中，$$D=diag(s_1^2,\dots,s_p^2)$$。

>注意：这里p维向量是正交基，否则的话，D将不是主对角线矩阵，而是一个普通的协方差矩阵。显然如果D为单位矩阵的话，马氏距离就变成了欧氏距离。

马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关。比如体重和身高数据，如果采用欧氏距离，则会由于量纲的不同，而无法比较。

由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。它的缺点是夸大了变化微小的变量的作用。

马氏距离的特点还包括：

1）马氏距离的计算是建立在总体样本的基础上的，这一点可以从上述协方差矩阵的解释中可以得出，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；

2）在计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离来代替马氏距离，也可以理解为，如果样本数小于样本的维数，这种情况下求其中两个样本的距离，采用欧式距离计算即可。

3）还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如A（3，4），B（5，6）；C（7，8），这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。

4）在实际应用中“总体样本数大于样本的维数”这个条件是很容易满足的，而所有样本点出现3）中所描述的情况是很少出现的，所以在绝大多数情况下，马氏距离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。

# 自相关&互相关&卷积

![](/images/article/Comparison_convolution_correlation.svg)

1.**自相关（Autocorrelation）**。这个最简单，就是平移之后，自己和自己比。显然当平移为0的时候，自相关值最大，因此这类操作通常用于信号的检测。信号接收端模拟发射端的信号序列，对实际接收到的信号进行相关操作，只有当两者的序列接近重合时，才会检测到信号峰值。

2.**互相关（Cross-correlation）**。检测两个序列的相似度，显然两者越相似，互相关值越大。这在统计学方面用的比较多。

3.**卷积（Convolution）**。卷积主要用于线性时不变系统的信号处理。相比于互相关操作，卷积有个旋转180度的操作，这里解释一下它的物理意义。

例如，当一个拳击选手遭到对方连续两次击打身体的同一部位时，第二次被击打时他感觉到的疼痛是第一次被击打所遗留的疼痛与第二次被击打的疼痛之和。即：

$$f(2)=f_1(2)+f_2(1)$$

其中，$$f_i(t)$$中，i表示第i次击打，t表示击打发生之后经过的时间。可以看出i和t的顺序正好是相反的，这也就是Convolution这个名词的本意。这里假设g为常数。

4.这三个操作在离散域最终都可以变为求和操作，也就是向量内积运算。我们一般使用$$a\cdot b$$或者$$\langle a,b\rangle$$表示向量的内积运算。即：

$$\langle a,b\rangle=a_0b_0+a_1b_1+\dots+a_nb_n$$

# 复变函数

1.复球面表示。

2.条件严格性。

点域：连续<可导（可微）<可解析

区域：连续<可导（可微）=可解析

3.函数可微的充要条件：Cauchy-Riemann Equations

4.复数在场论描述中的应用。

# 张量分析

在同构的意义下，第零阶张量（r = 0）为标量（Scalar），第一阶张量（r = 1）为向量（Vector），第二阶张量（r = 2）则成为矩阵（Matrix）。

《张量分析》，黄克智著。

>注：黄克智，1927年生，固体力学家。江西中正大学本科+清华硕士+莫斯科大学博士（因应召回国，放弃博士学位）。清华大学工程力学系教授、工程力学研究所所长，中国科学院院士。断裂力学领域权威。

# NLP

自然语言处理（Natural Language Processing）是深度学习的主要应用领域之一。

## 教程

http://cs224d.stanford.edu/

CS224d: Deep Learning for Natural Language Processing

http://web.stanford.edu/class/cs224n/syllabus.html

cs224d课程的课件

http://demo.clab.cs.cmu.edu/NLP/

CMU的NLP教程。该网页下方还有美国其他高校的NLP课程的链接。

http://ccl.pku.edu.cn/alcourse/nlp/

北京大学的NLP教程，特色：中文处理。

## 书籍

http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/Natural%20Language%20Processing%20with%20Python.pdf

《Natural Language Processing with Python》，Steven Bird、Ewan Klein、Edward Loper著。这本书的作者们创建了著名的NLTK工具库。

>注：Steven Bird，爱丁堡大学博士，墨尔本大学副教授。   
>http://www.stevenbird.net/about.html

>Ewan Klein，苏格兰人，哥伦比亚大学博士（1978年），爱丁堡大学教授。

>Edward Loper，宾夕法尼亚大学博士。

## 网站

http://www.52nlp.cn/

一个自然语言处理爱好者的群体博客。包括52nlp、rickjin、liwei等国内外华人大牛。

http://www.shareditor.com/bloglistbytag/?tagname=%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%81%9A%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA

实战课程：自己动手做聊天机器人

http://www.newsmth.net/bbsdoc.php?board=NLP

水木NLP板块

http://www.icst.pku.edu.cn/lcwm/

北京大学计算机科学技术研究所语言计算与互联网挖掘研究室

https://github.com/rockingdingo/deepnlp

NLP深度学习方面的代码库

https://liweinlp.com/

NLP专家李维的blog

## 工具

### Natural Language Toolkit(NLTK)

官网：

http://www.nltk.org/

可使用nltk.download()下载相关nltk官方提供的各种资源。

参考：

http://www.cnblogs.com/baiboy/p/nltk3.html

### OpenNLP

http://opennlp.apache.org/

### FudanNLP

https://github.com/FudanNLP/fnlp

### Stanford CoreNLP

http://stanfordnlp.github.io/CoreNLP/

### THUCTC

THUCTC(THU Chinese Text Classification)是由清华大学自然语言处理实验室推出的中文文本分类工具包。

http://thuctc.thunlp.org/

### gensim

gensim是Python语言的计算文本相似度的程序包。

http://radimrehurek.com/gensim/index.html

`pip install --upgrade gensim`

参考：

http://www.open-open.com/lib/view/open1444351655682.html

情感分析的新方法——基于Word2Vec/Doc2Vec/Python



### GloVe

GloVe:Global Vectors for Word Representation

https://nlp.stanford.edu/projects/glove/

### textsum

textsum是一个基于深度学习的文本自动摘要工具。

https://github.com/tensorflow/models/tree/master/textsum

http://www.jiqizhixin.com/article/1449

### jieba

https://github.com/fxsjy/jieba

### NLPIR

NLPIR汉语分词系统(又名ICTCLAS2013)，是中科院张华平博士的作品。官网：

http://ictclas.nlpir.org/

### HanLP

HanLP是一个目前留学日本的中国学生的作品。

官网：

http://hanlp.linrunsoft.com/

作者blog：

http://www.hankcs.com/

Github：

https://github.com/hankcs/HanLP/

从作者的名气来说，HanLP无疑是最低的，性能也不见得有多好。然而对于初学者来说，这却是最适合的工具。这主要体现在以下几个方面：

1.中文处理能力。NLTK和OpenNLP对中文支持非常差，这里不光是中文分词的问题，有些NLP算法需要一定的语言模型数据，但浏览NLTK官方的模型库，基本找不到中文模型数据。

2.jieba、IK之类的功能太单一，多数局限在中文分词方面领域。gensim、THUCTC专注于NLP的某一方面，也不是通用工具。

3.NLPIR和Stanford CoreNLP算是功能最强的工具包了。前者的问题在于收费不开源，后者的问题在于缺少中文文档。FudanNLP的相关文档较少，文档友好度不如HanLP。

4.HanLP在主页上提供了相关算法的blog，便于初学者快速掌握相关概念。其词典是明文发布，便于用户修改。HanLP执行时，会将明文词典以特定结构缓存，以提高执行效率。

>注：不要以为中文有分词问题，就比别的语言复杂，英文还有词根问题呢。。。每种语言都不简单。

### 其他

https://github.com/mozillazg/python-pinyin

python版的汉字转拼音软件

## 词性标注

http://jacoxu.com/ictpos3-0%E6%B1%89%E8%AF%AD%E8%AF%8D%E6%80%A7%E6%A0%87%E8%AE%B0%E9%9B%86/

ICTPOS3.0汉语词性标记集

## 有用的资料

http://www.newsmth.net/bbstcon.php?board=NLP&gid=15714

旺旺客服文本关键词提取

https://zhuanlan.zhihu.com/p/25928551

用深度学习（CNN RNN Attention）解决大规模文本分类问题-综述和实践

https://www.geekhub.cn/a/1163.html

宋睿华：好玩的文本生成

http://blog.csdn.net/sunlylorn/article/details/50607376

seq2seq模型

http://chuansong.me/n/521727951619

Seq2Seq的DIY简介

http://www.cnblogs.com/Determined22/p/6650373.html

DL4NLP——seq2seq+attention机制的应用：文档自动摘要（Automatic Text Summarization）

http://www.cnblogs.com/maybe2030/p/5427148.html

文本深度表示模型——word2vec&doc2vec词向量模型

https://www.zhihu.com/question/29978268

如何用word2vec计算两个句子之间的相似度？

https://github.com/Gii16/TreeDrawer

A small tool to draw a only-text tree as the result of stanford coreNLP

http://www.cnblogs.com/CheeseZH/p/5768389.html

依存句法分析与语义依存分析的区别

http://www.taodocs.com/p-4795349.html

基于大规模问答语料的问题检索系统

https://wenku.baidu.com/view/d38ab1e8856a561252d36fab.html

短文本相似度计算在用户交互式问答系统中的应用

https://zhuanlan.zhihu.com/p/26461511

关于WordNet，我的一些用法和思路


