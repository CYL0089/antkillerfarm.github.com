---
layout: post
title:  图像处理理论（五）——从BOW到SPM, ILSVRC 2010考古, Python（三）
category: graphics 
---

# SIFT（续）

## 总结

SIFT大概算是最复杂的算子了，上文仅是概括描述，省略了大量公式及其推导，但仍然花了很大篇幅才讲完。可以想见，设计一个比SIFT还好的算子会有多么困难，人工设计特征走到这里，差不多也就到头了，后面的天下是属于DL的。

## 参考

http://blog.csdn.net/zddblog/article/details/7521424

SIFT算法详解

# 从BOW到SPM

## BOW

Bag-of-words模型是信息检索领域常用的文档表示方法。在信息检索中，BOW模型假定对于一个文档，忽略它的单词顺序和语法、句法等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的，不依赖于其它单词是否出现。

为了表示一幅图像，我们可以将图像看作文档，即若干个“视觉词汇”的集合，同样的，视觉词汇相互之间没有顺序。

![](/images/article/cv_bow.jpg)

由于图像中的词汇不像文本文档中的那样是现成的，我们需要首先从图像中提取出相互独立的视觉词汇，这通常需要经过三个步骤：

（1）特征检测。

（2）特征表示。

（3）单词本的生成。

而SIFT算法是提取图像中局部不变特征的应用最广泛的算法，因此我们可以用SIFT算法从图像中提取不变特征点，作为视觉词汇，并构造单词表，用单词表中的单词表示一幅图像。



参考：

http://blog.csdn.net/v_JULY_v/article/details/6555899

SIFT算法的应用--目标识别之Bag-of-words模型

## SPM

http://blog.csdn.net/chlele0105/article/details/16972695

SPM:Spatial Pyramid Matching for Recognizing Natural Scene Categories空间金字塔匹配

http://blog.csdn.net/jwh_bupt/article/details/9625469

Spatial Pyramid Matching 小结

# ILSVRC 2010考古

ILSVRC 2010的冠军是NEC和UIUC的联合队伍。这也是DL于2012年大放光彩之前比较杰出的成果。虽然现在它通常作为反面教材，出现在与DL的对比场景中，然而不可否认的是，它仍然是一个算法的杰作。

>林元庆，清华大学硕士+宾夕法尼亚大学博士（2008年）。原百度研究院院长。

![](/images/article/ILSVRC_2010.png)

上图是NEC算法的基本流程图。这里不打算描述整个算法，而仅对其中涉及的术语做一个解释。

## LBP

http://blog.csdn.net/dujian996099665/article/details/8886576

opencv学习之（三）-LBP算法的研究及其实现

https://mp.weixin.qq.com/s/iFlnZ8z5baUdWCZxIGkq5g

机器学习实战——LBP特征提取

# 经典目标跟踪算法

camshift、meanshift、Kalman filter、particle filter、Optical flow、TLD、KCF、Struck

## Meanshift

参考：

http://www.cnblogs.com/liqizhou/archive/2012/05/12/2497220.html

Meanshift，聚类算法

https://wenku.baidu.com/view/0d9eb876a417866fb84a8eb2.html

mean-shift算法概述

http://www.cnblogs.com/cfantaisie/archive/2011/06/10/2077188.html

meanshift聚类

## Camshift

参考：

http://blog.sina.com.cn/s/blog_5d1476580101a57j.html

Camshift算法

http://blog.163.com/thomaskjh@126/blog/static/370829982010113133152722/

CAMSHIFT原理

https://wenku.baidu.com/view/59596ac42cc58bd63186bd37.html

Camshift算法原理

## Optical flow

http://www.cnblogs.com/walccott/p/4956858.html

Horn-Schunck光流法

http://blog.csdn.net/u014568921/article/details/46638557

目标跟踪之Lukas-Kanade光流法

http://blog.csdn.net/zouxy09/article/details/8683859

光流Optical Flow介绍与OpenCV实现

http://www.cnblogs.com/gnuhpc/archive/2012/12/04/2802124.html

Lucas–Kanade光流算法

http://www.cnblogs.com/dzyBK/p/5096860.html

光流算法：Brox算法

http://www.cnblogs.com/quarryman/p/optical_flow.html

图像分析之光流之经典

## Particle filter

http://www.cvvision.cn/6002.html

基于粒子滤波器的目标跟踪算法及实现

http://www.cnblogs.com/zjb0823/p/3806333.html

运动目标跟踪算法综述

https://wenku.baidu.com/view/6554ba7402768e9951e73864.html

基于粒子滤波的视频目标追踪

http://www.cnblogs.com/feisky/archive/2009/11/10/1600086.html

粒子滤波概述

http://www.cnblogs.com/yangyangcv/archive/2010/05/23/1742263.html

基于粒子滤波的物体跟踪

https://www.zhihu.com/question/25371476

怎样从实际场景上理解粒子滤波（Particle Filter）？

http://freemind.pluskid.org/machine-learning/hmm-kalman-particle-filtering

漫谈HMM：Kalman/Particle Filtering

## TLD

http://blog.csdn.net/zouxy09/article/details/7893011

TLD（Tracking-Learning-Detection）学习与源码理解系列文章

http://blog.csdn.net/carson2005/article/details/7647500

比微软kinect更强的视频跟踪算法--TLD跟踪算法介绍

http://www.cnblogs.com/lxy2017/p/3927456.html

TLD（Tracking-Learning-Detection）一种目标跟踪算法

# YUV & YCbCr & RGB

在RGB颜色空间中，红，绿，蓝是基本元素。RGB格式是显示器通常使用的格式。

然而，人类视觉系统(HVS)相比亮度来说对于颜色不是那么敏感的。通过把亮度与颜色信息分离，并对亮度值取更高的分辨率可以更有效地表示一个颜色图像。这就是所谓的YUV颜色空间。Y表示亮度，B－Y（即U）、R－Y（即V）是色差信息。

实际使用中，为了更有效率，又定义了YCbCr格式。两个Y、Cb和U、Cr和V的含义相同，但是表示方法不同。YCbCr在YUV的基础上，进行了比例和偏置变换。

相关的定义及转换标准是：ITU-R Recommendation BT.601（标清）和ITU-R Recommendation BT.709（高清）。

# python

## python调用GTK函数的一般规则

众所周知，GTK库是用C语言编写的，其相关的文档提供的也是C函数的接口，python接口的文档相对较少，获得也不太容易。因此有必要掌握一下这方面基本的命名规则。（这里只讲方法，对深层次的调用原理不做探究。）

1）类初始化函数

C:`GtkWidget * gtk_button_new (void);`

python:`button = Gtk.Button()`

这里除了把C函数式的写法，替换成python的类的写法之外，并无其他差异。

2）普通类成员函数

C:`void gtk_container_add (GtkContainer *container, GtkWidget *widget);`

python:`button.add(image)`

可以看出，C函数的第一个表示self类指针的参数被省略掉了。

3）回调函数

这里以按钮的click事件回调为例：

C:`void user_function (GtkButton *button, gpointer user_data)`

python:

`button.connect("clicked", self.playToggled)`

`def playToggled(self, w):`

这里的情况要复杂的多。首先在事件回调注册的时候，由于我们并没有给user_data赋值，因此照理说playToggled函数，只有button这一个参数。但实际传递给playToggled函数的有两个参数，self是一个用于占位的参数，w对应button，没有东西对应user_data。

4）用于输出的参数

C:`void gst_message_parse_error (GstMessage *message, GError **gerror, gchar **debug);`

python:`err, debug = message.parse_error()`

这个例子中的gerror和debug都是用于输出的参数。如果只想得到其一，还可用以下方法:

`debug = message.parse_error()[2]`

5）枚举类型

C:

{% highlight c %}
enum GstSeekFlags
{
  GST_SEEK_FLAG_FLUSH,
  ...
}
{% endhighlight %}

python:`Gst.SeekFlags.FLUSH`

6）宏常量

C:`#define GST_CLOCK_TIME_NONE ((GstClockTime) -1)`

python:`Gst.CLOCK_TIME_NONE`

7）类型转换

大多数情况下，类型转换自动完成，并无什么问题。这里只讨论特殊的情况。

C:

{% highlight c %}
gst_element_seek (pipeline, 1.0, GST_FORMAT_TIME, GST_SEEK_FLAG_FLUSH,
                         GST_SEEK_TYPE_SET, time_nanoseconds,
                         GST_SEEK_TYPE_NONE, GST_CLOCK_TIME_NONE))
{% endhighlight %}

python:

{% highlight python %}
self.player.seek(1.0, Gst.Format.TIME, Gst.SeekFlags.FLUSH, Gst.SeekType.SET,\
                 time_nanoseconds, Gst.SeekType.NONE, Gst.CLOCK_TIME_NONE)
{% endhighlight %}

这里首先按照一般的方法，将C代码转化成python代码。但是运行之后，产生如下错误：

`OverflowError: long too big to convert`

究其原因，gst_element_seek的最后一个参数是uint64类型的。但python原生并不支持该类型，而是将之转换成int类型（实际上是int64类型），这样的话，GST_CLOCK_TIME_NONE的值显然超出了int64所能表示的范围。

解决的办法是使用ctypes库，将

`Gst.CLOCK_TIME_NONE`

改为

`c_long(Gst.CLOCK_TIME_NONE).value`

## 参考

https://mp.weixin.qq.com/s/jHnkmpnOz1LApVCLJL-8Tw

如何写出优雅又地道的Python代码？


