---
layout: post
title:  数学狂想曲（四）——软件滤波算法, 玻尔兹曼分布
category: math 
---

# 矩阵&向量的积（续）

## 向量的笛卡尔积

Cartesian product实际上是集合论中的概念。

$$A\times B = \{1,2\} \times \{3,4\} = \{(1,3), (1,4), (2,3), (2,4)\}$$

## 向量的张量积

Tensor product，又称outer product。

$$\begin{align}\mathbf{u} \otimes \mathbf{v} = \mathbf{u} \mathbf{v}^\mathrm{T}
= \begin{bmatrix}u_1 \\ u_2 \\ u_3 \\ u_4\end{bmatrix}
\begin{bmatrix}v_1 & v_2 & v_3\end{bmatrix}
= \begin{bmatrix}u_1v_1 & u_1v_2 & u_1v_3 \\ u_2v_1 & u_2v_2 & u_2v_3 \\ u_3v_1 & u_3v_2 & u_3v_3 \\ u_4v_1 & u_4v_2 & u_4v_3\end{bmatrix}\end{align}$$

可以看出，Tensor product和Cartesian product，虽然形式上都是各向量的组合，然而前者是2维的，而后者是1维的。

## 矩阵的张量积

张量积推广到矩阵，即所谓Kronecker product。

>注：Leopold Kronecker，1823～1891，德国数学家。柏林大学博士和教授。导师Dirichlet。他最牛逼的地方是，当Riemann去世的时候，拒绝了哥廷根大学的offer。而这个位置的前任分别是：Carl Gauss、Dirichlet、Riemann。

$$\mathbf{A}\otimes\mathbf{B} = \begin{bmatrix} a_{11} \mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ \vdots & \ddots & \vdots \\ a_{m1} \mathbf{B} & \cdots & a_{mn} \mathbf{B} \end{bmatrix}$$

## Hadamard product

又叫Schur product或entrywise product。

>注：Jacques Salomon Hadamard，1865～1963，法国数学家。巴黎高等师范学校博士，波尔多大学教授。他曾于1936年访华，执教于清华大学。中国偏微分方程研究事业的主要创始人之一——吴新谋教授，就是他的学生。

$$\left(\begin{array}{ccc}
    \mathrm{a}_{11} & \mathrm{a}_{12} & \mathrm{a}_{13}\\
    \mathrm{a}_{21} & \mathrm{a}_{22} & \mathrm{a}_{23}\\
    \mathrm{a}_{31} & \mathrm{a}_{32} & \mathrm{a}_{33}
  \end{array}\right) \circ \left(\begin{array}{ccc}
    \mathrm{b}_{11} & \mathrm{b}_{12} & \mathrm{b}_{13}\\
    \mathrm{b}_{21} & \mathrm{b}_{22} & \mathrm{b}_{23}\\
    \mathrm{b}_{31} & \mathrm{b}_{32} & \mathrm{b}_{33}
  \end{array}\right) = \left(\begin{array}{ccc}
    \mathrm{a}_{11}\, \mathrm{b}_{11} & \mathrm{a}_{12}\, \mathrm{b}_{12} & \mathrm{a}_{13}\, \mathrm{b}_{13}\\
    \mathrm{a}_{21}\, \mathrm{b}_{21} & \mathrm{a}_{22}\, \mathrm{b}_{22} & \mathrm{a}_{23}\, \mathrm{b}_{23}\\
    \mathrm{a}_{31}\, \mathrm{b}_{31} & \mathrm{a}_{32}\, \mathrm{b}_{32} & \mathrm{a}_{33}\, \mathrm{b}_{33}
  \end{array}\right)$$

# 软件滤波算法

## 限幅滤波法

方法：根据经验判断，确定两次采样允许的最大偏差值（设为A），每次检测到新值时判断：如果本次值与上次值之差<=A，则本次值有效，如果本次值与上次值之差>A，则本次值有效，放弃本次值，用上次值代替本次值。

优点：能有效克服因偶然要素惹起的脉冲干扰。

缺点：无法抑制那种周期性的干扰，平滑度差。

## 中位值滤波法

方法：连续采样N次（N取奇数），把N次采样值按大小陈列，取中位值（第$$\frac{(N-1)}{2}$$个值）为本次有效值。

优点：能有效克服因偶然要素惹起的波动干扰，对变化缓慢的被测参数有良好的滤波效果。

缺点：对快速变化的参数不宜。

## 算术平均滤波法

方法：连续取N个采样值进行算术平均运算，N值较大时：信号平滑度较高，但灵敏度较低；N值较小时：信号平滑度较低，但灵敏度较高。

优点：适用于对普通具有随机干扰的信号进行滤波，这样信号的特点是有一个平均值，信号在某一数值范围附近上下波动。

缺点：对于测量速度较慢或要求数据计算速度较快的实时控制不适用，比较浪费RAM 。

## 递推平均滤波法（又称滑动平均滤波法）

方法：把连续取的N个采样值看成一个队列，队列的长度固定为N，每次采样到一个新数据放入队尾，并扔掉原来队首的一次数据(先进先出) 。把队列中的N个数据进行算术平均运算，就可获得新的滤波结果。

优点：对周期性干扰有良好的抑制效用，平滑度高，适用于高频振荡系统。

缺点：灵敏度低，对偶然出现的脉冲性干扰的抑制效用较差，不易消弭由于脉冲干扰所引起的采样值偏差，不适用于脉冲干扰比较严重的场合，比较浪费RAM。

## 中位值平均滤波法（又称防脉冲干扰平均滤波法）

方法：相当于“中位值滤波法”+“算术平均滤波法”，连续采样N个数据，去掉一个最大值和一个最小值，然后计算N-2个数据的算术平均值。。

优点：融合了两种滤波法的优点，对于偶然出现的脉冲性干扰，可消弭由于脉冲干扰所惹起的采样值偏差。

缺点：测量速度较慢，和算术平均滤波法一样，比较浪费RAM。

## 限幅平均滤波法

方法：相当于“限幅滤波法”+“递推平均滤波法”，每次采样到的新数据先进行限幅处理，再送入队列进行递推平均滤波处理。

优点：融合了两种滤波法的优点，对于偶然出现的脉冲性干扰，可消弭由于脉冲干扰所惹起的采样值偏差。

缺点：比较浪费RAM。

## 一阶滞后滤波法

方法：取a=0~1，本次滤波结果=(1-a)*本次采样值+a*上次滤波结果。

优点：对周期性干扰具有良好的抑制造用，适用于波动频率较高的场合。

缺点：相位滞后，灵敏度低，滞后程度取决于a值大小，不能消弭滤波频率高于采样频率的1/2的干扰信号。

## 加权递推平均滤波法

方法：这是对递推平均滤波法的改进，即不同时刻的数据加以不同的权，通常是，越接近现时刻的材料，权取得越大，给予新采样值的权系数越大，则灵敏度越高，但信号平滑度越低。

优点：适用于有较大纯滞后事件常数的对象和采样周期较短的系统。

缺点：对于纯滞后事件常数较小，采样周期较长，变化缓慢的信号，不能迅速反应系统当前所受干扰的严重程度，滤波效果差。

## 消抖滤波法

方法：设置一个滤波计数器，将每次采样值与当前有效值比较：如果采样值等于当前有效值，则计数器清零。如果采样值不等于当前有效值，则计数器+1，并判断计数器能否>=下限N(溢出)，如果计数器溢出，则将本次值交换当前有效值，并清计数器。

优点：对于变化缓慢的被测参数有较好的滤波效果，可避免在临界值附近控制器的反复开/关跳动或显示器上数值抖动。

缺点：对于快速变化的参数不宜，如果在计数器溢出的那一次采样到的值恰好是干扰值，则会将干扰值当作有效值导入系统。

## 限幅消抖滤波法

方法：相当于“限幅滤波法”+“消抖滤波法”，先限幅后消抖。

优点：承继了“限幅”和“消抖”的优点，改进了“消抖滤波法”中的某些缺陷，避免将干扰值导入系统。

缺点：对于快速变化的参数不宜。

## IIR数字滤波

方法：确定信号带宽，滤之。

$$Y(n)=a_1*Y(n-1)+a_2*Y(n-2)+...+a_k*Y(n-k)+$$

$$b_0*X(n)+b_1*X(n-1)+b_2*X(n-2)+...+b_k*X(n-k)$$

优点：高通，低通，带通，带阻任意。design简单(用matlab）。

缺点：运算量大。

# 玻尔兹曼分布

Boltzmann distribution（又叫Gibbs distribution）本来是脱胎于热力学和统计力学的分布，但在量子力学和机器学习领域也得到了广泛的应用。

>注：Ludwig Eduard Boltzmann，1844～1906，奥地利物理学家，统计力学的奠基人之一。维也纳大学博士，先后执教于格拉茨大学、维也纳大学、慕尼黑大学和莱比锡大学。英国皇家学会会员。

## 信息熵

设概率实验X有n个可能的独立结局，即n个随机事件$$A_1,\dots,A_n$$，每个随机事件的概率为$$p_1,\dots,p_n$$。则信息熵的定义为：

$$H_n=-k\sum_{i=1}^{n}p_i\ln p_i\tag{1}$$

其中k为一正常数。可以看出$$H_n$$实际上是$$p_i$$的泛函。

以下我们主要从信息熵的角度出发研究玻尔兹曼分布。但不能忽视的是，玻尔兹曼分布最早脱胎于热力学，而信息熵与热力学统计物理中的熵，虽然数学形式一致，但本质是不同的。两者相差一个玻尔兹曼常数。参见：

https://www.zhihu.com/question/20992022/answer/50458123

信息熵与热力学统计物理中的熵有什么区别和联系？

>注：信息熵的定义有多种，公式1给出的是Shannon熵的定义，除此之外还有von Neumann熵、Renyi熵等。以下如无特殊指出，信息熵均指Shannon熵。

## 最大信息熵原理

事实上，有些随机事件其概率往往不可能直接计算，平常我们对具体问题作出处理时，掌握的仅仅是一些与随机事件有关的随机变量的统计平均值，以及某些其他制约条件。而当一个随机变量的平均值给定时，还可以有多种概率分布与之相容。现在的问题是如何从这些相容的概率分布中挑选出“最可几”的分布来作为实际上的分布。显然，要做到这点，必须有个挑选标准，最大信息熵原理就可作为这种挑选标准。

>注：最可几分布（Most Probable Distribution），又称最概然分布。它表征在特定系统的特定状态下，出现概率最大的分布。从它的定义可以看出，它实际上并不是某种具体的分布，而更像是符合某一特定目标的分布。   
>比如，麦克斯韦-玻尔兹曼分布、费米-狄拉克分布和玻色-爱因斯坦分布，是热力学常见的三种最可几分布。但它们所处的系统，以及系统的状态各不相同。

最大熵原理是1957年由E.T.Jaynes提出的，其主要思想是，在只掌握关于未知分布的部分知识时，应该选取符合这些知识但熵值最大的概率分布。其实质就是，在已知部分知识的前提下，关于未知分布最合理的推断就是符合已知知识最不确定或最随机的推断，这是我们可以作出的唯一不偏不倚的选择，任何其它的选择都意味着我们增加了其它的约束和假设，这些约束和假设根据我们掌握的信息无法作出。

>注：Edwin Thompson Jaynes，1922～1998，美国统计学家。普林斯顿大学博士，华盛顿大学教授。

假定系统涉及的M个随机变量$$f_i^{(j)}(j=1,\dots,M)$$的期望值$$F_j$$已知，即：

$$F_j=\sum_{i=1}^{n}f_i^{(j)}p_i\tag{2}$$

其中$$f_i^{(j)}$$表示第j个随机变量在状态i中的取值。$$p_i$$满足归一化条件：

$$\sum_{i=1}^{n}p_i=1\tag{3}$$

将公式2和公式3所述的两个约束，代入公式1，可得如下Lagrange乘子：

$$J(p_i)=H_n/k-\alpha(\sum_{i=1}^{n}p_i-1)-\sum_{j=1}^{M}\beta_j(\sum_{i=1}^{n}f_i^{(j)}p_i-F_j)
\\=-\sum_{i=1}^{n}p_i\ln p_i-\alpha\sum_{i=1}^{n}p_i+\alpha-\sum_{j=1}^{M}\sum_{i=1}^{n}\beta_jf_i^{(j)}p_i+\sum_{j=1}^{M}\beta_jF_j$$

对$$p_i$$求导，可得：

$$\frac{\partial J(p_i)}{\partial p_i}=-\ln p_i-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)}$$

令上式等于0，可得：

$$p_i=\exp(-1-\alpha-\sum_{j=1}^{M}\beta_jf_i^{(j)})\tag{4}$$

公式4就是一般情况下的最大信息熵公式。


