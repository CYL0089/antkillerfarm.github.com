---
layout: post
title:  语音识别（一）——概述, 基本框架, Microphone Array
category: graphics 
---

# 概述

虽然现在的语音识别中，DL已经应用的非常广泛了，但是语音识别终究还是有一些领域知识的，将之归类为DL或者ML，似乎都不妥当。但是本人现状又不大可能专门研究这个领域，形成系列文章，因此暂时归入graphics分类，用以描述speech recognition的领域知识和传统方法。

说起来还是要感谢DL，不然按照传统的行业划分，几乎不会有人同时研究CV和SR。DL的出现，实际上大大降低了算法的领域迁移成本，领域知识的重要性相对下降了。

## 学校

SR领域最牛的高校主要是美国的CMU、Johns Hopkins University和英国的Cambridge University。

## 书籍

《Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition》，Daniel Jurafsky & James H. Martin著。

>Daniel Jurafsky，1962年生，UCB本科（1983）+博士（1992）。斯坦福大学教授。   
>个人主页：   
>https://web.stanford.edu/~jurafsky/

>James H. Martin，哥伦比亚大学本科+UCB博士。University of Colorado Boulder教授。   
>个人主页：   
>http://www.cs.colorado.edu/~martin/

>这本书比较老了（1999年），但毕竟是本1000页左右的书，传统方法该说的基本都说了。DL方法目前还没有一本类似的书，主要还是看论文。

## 教程

http://tts.speech.cs.cmu.edu/courses/11492/schedule.html

Speech Processing。CMU的这个教程主要包含ASR（Automatic Speech Recognition）、TTS（Text To Speech）和SDS（Spoken Dialog Systems）等三方面的内容。

>Alan W Black，苏格兰计算机科学家。Coventry University本科（1984）+University of Edinburgh硕博（1984,1993）。CMU教授。语音处理专家。   
>个人主页：
>http://www.cs.cmu.edu/~awb/   
>他的主页上有好多Speech、NLP方面的教程。他本人长得太像Java之父James Gosling了。

http://web.stanford.edu/class/cs224s/index.html

CS224S / LINGUIST285 - Spoken Language Processing。Stanford的教程相对比较新，DL涉及的比较多。

## blog

http://www.cnblogs.com/welen/

https://blog.csdn.net/weiqiwu1986

上面两个都是welen的blog，而且内容貌似还不重复。。。

http://blog.csdn.net/xmdxcsj

一个语音识别的blog

https://blog.csdn.net/shichaog

一个语音识别+Kaldi的blog

https://blog.csdn.net/quhediegooo/

一个语音识别的blog

https://blog.csdn.net/dearwind153/article/category/6506891

这哥们的blog很杂，这是语音相关的专栏

## 项目

https://en.wikipedia.org/wiki/List_of_speech_recognition_software

List of speech recognition software

https://mp.weixin.qq.com/s/LsVhMaHrh8JgfpDra6KSPw

横向对比5大开源语音识别工具包

https://github.com/lingochamp/kaldi-ctc

英语流利说开源的kaldi-ctc

https://zhuanlan.zhihu.com/p/23177950

kaldi-ctc: CTC End-to-End ASR

## HTK

Hidden Markov Model Toolkit是Cambridge University开发的语音识别的工具包。它是GMM-HMM时代最为流行的语音识别工具，但缺乏对DL的支持。

官网：

http://htk.eng.cam.ac.uk/

## CMU Sphinx

CMU Sphinx是李开复的博士课题项目，后来成为了CMU的长期项目。洪小文、黄学东也先后参与过。该项目比较早的将HMM应用于语音识别，这在当时算是一个重大创新。

>李开复，1961年生，Columbia University本科（1983）+CMU博士（1988）。先后供职于Apple、SGI、Microsoft、Google。现为创新工场董事长。

>洪小文，1963年生，台湾大学本科+CMU博士。先后供职于Apple、Microsoft，现为微软亚洲研究院院长。

>黄学东，1962年生，湖南大学本科（1982）+清华大学硕士（1984）+University of Edinburgh博士（1989）。现为微软首席语音科学家。

>Raj Reddy，1937年生，印度裔美国计算机科学家。印度University of Madras本科（1958）+澳大利亚University of New South Wales硕士（1960）+Stanford University博士。CMU教授，首位亚裔图灵奖得主（1994）。   
>他还是印度Rajiv Gandhi University of Knowledge Technologies创始人和International Institute of Information Technology, Hyderabad主席。   
>他是李开复、洪小文的博士导师，黄学东的博士后导师。

官网：

https://cmusphinx.github.io/

注意：还有一个类似Elasticsearch的文本搜索引擎也叫Sphinx。它的官网是：

http://sphinxsearch.com/

## 公司

http://www.aispeech.com/

思必驰

http://www.soundai.com/

声智科技。偏重于语音信号处理。

https://zhuanlan.zhihu.com/chenxl

声智科技创始人陈孝良的专栏

## 数据集

http://www.speech.cs.cmu.edu/databases/an4/

The CMU Audio Databases。这个数据集非常老了（1991年），只有64M。

http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz

TensorFlow提供的Speech Commands Datasets

还有相关的工具：

https://github.com/petewarden/extract_loudest_section

抽取一段wav文件中声音最大的那部分

https://www.kaggle.com/davids1992/speech-representation-and-data-exploration/notebook

包含对Speech Commands Datasets的数据处理过程的blog

https://catalog.ldc.upenn.edu/LDC93S1

TIMIT数据集（收费）

https://mp.weixin.qq.com/s/w9_D1_VVhk9md4RANaipDg

Mozilla开源语音识别模型和世界第二大语音数据集

http://www.voxforge.org/

VoxForge是一个非常活跃的众包语音识别数据库和经过训练的模型库

http://pan.baidu.com/s/1dEhUghz

清华大学语音和语言技术研究中心（CSLT）公开的数据集

# 基本框架

语音识别系统主要有四部分组成：信号处理和特征提取、声学模型、语言模型（Language Model, LM）和解码器(Decoder)。

![](/images/img2/speech.png)

**信号处理和特征提取**部分以音频信号为输入，通过消除噪音、信道失真等对语音进行增强，将语音信号从时域转化到频域，并为后面的声学模型提取合适的特征。

**声学模型**将声学和发音学的知识进行整合，以特征提取模块提取的特征为输入，生成声学模型得分。

**语言模型**估计通过重训练语料学习词之间的相互概率，来估计假设词序列的可能性，也即语言模型得分。如果了解领域或者任务相关的先验知识，语言模型得分通常可以估计得更准确。

**解码器**对给定的特征向量序列和若干假设词序列计算声学模型得分和语言模型得分，将总体输出分数最高的词序列作为识别结果。

![](/images/img2/speech_2.png)

上图是一个实际的recognition engine的结构图。其原文地址：

https://eprints.lib.hokudai.ac.jp/dspace/bitstream/2115/39654/1/MP-SS1-4.pdf

王赟写的《语音识别技术的前世今生》，写的非常好，下载地址：

https://zhihu-live.zhimg.com/0af15bfda98f5885ffb509acd470b0fa

>王赟，清华本科（2010）+CMU在读博士。   
>个人主页：   
>http://www.cs.cmu.edu/~yunwang/

下面是网友写的注解版本：

http://www.cnblogs.com/lyu0709/p/6929659.html

《语音识别的前世今生：GMM+HMM & 深度学习》讲座笔记

http://www.cnblogs.com/lyu0709/p/6929692.html

《语音识别的前世今生》Q&A

参考：

https://blog.csdn.net/by21010/article/details/51506292

语音识别系统结构——鸟瞰

http://www.cnblogs.com/welen/p/7489504.html

语音识别概述

https://www.zhihu.com/question/20398418

语音识别的技术原理是什么？

# Microphone

麦克风作为业界通俗的一种叫法，是英文Microphone的音译名称，国内的称呼乱一些，有时候也简单称作话筒，香港和台湾地区也会称作微音器、拾音器。麦克风的中文学术名称是传声器，这是一种将声音转换成电子信号的换能器，即把声信号转成电信号，这其实和光电转换的原理是完全一致的。 

消费级市场的麦克风基本都是标量麦克风，也就说只能采集单一的物理量信息——声压。声压是指声波通过媒质时，由振动所产生的压强改变量，也可以理解为声音的幅度或者强度。声压常用字母"p"表示，单位是帕斯卡（符号Pa）。声压的帕斯卡单位由于不方便记忆（比如$$20\times 10^{-6}$$Pa~20Pa），一般就以对数尺衡量有效声压相对于一个基准值的大小来表示，即声压级，其单位是分贝（符号dB）。

人类对于1KHz的声音的听阈为$$20\times 10^{-6}$$Pa，通常以此作为声压级的基准值。这样讲可能晦涩难懂，我们来简单的类比一下：人类的呼吸声压是$$60\times 10^{-6}$$左右，声压级大约10dB，火箭发射的声压是4000Pa左右，声压级大约165dB，闪光弹的声压超过1万Pa，声压级大约175dB。

为了描述麦克风的性能，有几个性能指标是非常关键的，这包括了灵敏度、指向性、频率响应、阻抗、动态范围、信噪比、最大声压级（或AOP，声学过载点）、一致性等。

现在麦克风阵列主要使用的是**数字MEMS麦克风**，其最长尺寸仅有3.76MM。MEMS麦克风也是手机中大量使用的传感器件，一般手机至少有2个以上这类麦克风。MEMS麦克风实际上只是工艺上的改进，其原理依然属于电容式麦克风。

与MEMS麦克风直接PK的，就是**驻极体麦克风**。它的性能更优秀，但一致性不如MEMS麦克风，因此主要用在单麦上。而麦克风阵列一般都是MEMS麦克风。

被淘汰的技术：带式麦克风、碳精麦克风（老式电话）。

先进技术：压电麦克风、光纤麦克风、激光麦克风、矢量麦克风。

Microphone的主要厂商：

1.Knowles 美国

2.瑞声(AAC) 中国

3.歌尔(Goertek) 中国

4.BSE 韩国

5.ST 欧洲

参考：

https://zhuanlan.zhihu.com/p/27610503

盘点麦克风技术及市场，远场语音交互如何选型麦克风

# Microphone Array

麦克风阵列(Microphone Array)，从字面上，指的是麦克风的排列。也就是说由一定数目的声学传感器(一般是麦克风)组成，用来对声场的空间特性进行采样并处理的系统。

![](/images/img2/Echo.png)

上图是Amazon Echo所采用的6+1麦克风阵列。

根据声波传导理论，利用多个麦克风收集到的信号可以将某一方向传来的声音增强或抑制。利用这种方法，麦克风阵列可以将噪声环境中特定声音信号有效的增强。由于麦克风阵列技术具有很好的抑制噪声和语音增强的能力，又不需要麦克风时刻指向声源方向，因此在语音处理领域具有非常好的前景，可以用在非常广的应用领域。


