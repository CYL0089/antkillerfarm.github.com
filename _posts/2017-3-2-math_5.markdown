---
layout: post
title:  数学狂想曲（五）——概率分布（2）
category: math 
---

# 玻尔兹曼分布（续）

## 相关名词辨析

这里有三个非常容易混淆的名词需要解释。

**Maxwell–Boltzmann statistics**是研究热平衡状态下无相互作用的物体粒子的能量均值分布的统计学分支。

**Boltzmann distribution**表征一个包含各种能量状态的系统中粒子的概率分布。

**Maxwell-Boltzmann distribution**表征粒子的能级和速度的分布。

简而言之，Maxwell–Boltzmann statistics是基于特定假设的统计学分支。而后两者则是具体情况下的概率分布。

与Maxwell–Boltzmann statistics相对应的概念是Bose–Einstein statistics和Fermi–Dirac statistics。后两者分别对应玻色子和费米子的热力学统计。

参考：

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_statistics

https://en.wikipedia.org/wiki/Boltzmann_distribution

https://en.wikipedia.org/wiki/Maxwell–Boltzmann_distribution

## 玻尔兹曼分布的推导

Maxwell–Boltzmann statistics研究的对象是一个由N（N很大）个微观粒子组成的近独立体系，体系的总能量E等于各个粒子能量之和。通常微观粒子的状态和能量是量子化的（即所谓能级），每一能级上又具有若干个量子态。为此，可引入下列符号：

| 名称 | 符号 |
|:--:|:--:|
| 单粒子状态编号 | $$1,\dots,n$$ |
| 能级 | $$E_1,\dots,E_n$$ |
| 量子态数 | $$g_1,\dots,g_n$$ |

令$$m=g_1+\dots+g_n$$，$$p_s$$为任何一个确定的微观粒子处于第s个量子态上的概率。因此，相应的信息熵和概率分布的归一化条件为：

$$H_m=-k\sum_{s=1}^{m}p_s\ln p_s\tag{5}$$

$$\sum_{s=1}^{m}p_s=1\tag{6}$$

由于N是个大数，可以认为第s个量子态上的微观粒子数为$$N_s=Np_s$$，体系的总能量为：

$$E=\sum_{s=1}^{m}E_sN_s=\sum_{s=1}^{m}E_sNp_s\tag{7}$$

和公式4的推导过程类似，Maxwell–Boltzmann statistics条件下的信息熵最大值的概率分布为：

$$p_s=\exp(-1-\alpha-\beta E_s)\tag{8}$$

由$$p_s$$的归一化条件可得：

$$p_s=\frac{1}{Z}e^{-\beta E_s}\tag{9}$$

其中：

$$Z=\sum_{s=1}^{m}e^{-\beta E_s}$$

于是，当体系处于热平衡状态时，第s个量子态上的粒子数可表为：

$$N_s=Np_s=\frac{N}{Z}e^{-\beta E_s}\tag{10}$$

考虑到同一单粒子能级$$E_i$$上的$$g_i$$个量子态均具有相同的能量$$E_i$$，因此：

$$N_i=N_sg_i\tag{11}$$

由气体分子的速度分布可得：

$$\beta=\frac{1}{kT}\tag{12}$$

其中，k是Boltzmann常数，T是热力学温度。

将公式11、12代入公式10可得：

$$N_i=\frac{Ng_ie^{-E_i/kT}}{\sum_{i=1}^{n}g_ie^{-E_i/kT}}$$

上式就是Boltzmann distribution的PDF。

## 参考

http://www.taodocs.com/p-62206829.html

《受限波尔兹曼机简介》张春霞著

http://wenku.baidu.com/view/ccb7956db84ae45c3b358ca7.html

波尔兹曼统计分布律的推导的新方法

http://www.cs.toronto.edu/~fritz/absps/cogscibm.pdf

A Learning Algorithm for Boltzmann Machines

http://www.cs.toronto.edu/~hinton/absps/dbm.pdf

Deep Boltzmann Machines

http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf

A Practical Guide to Training Restricted Boltzmann Machines

http://www.cnblogs.com/tianchi/archive/2013/03/14/2959716.html

Boltzmann machine

http://www.cnblogs.com/kemaswill/p/3269138.html

基于受限玻尔兹曼机(RBM)的协同过滤

http://www.cnblogs.com/kemaswill/p/3203605.html

受限波尔兹曼机简介

https://mp.weixin.qq.com/s/MqnJ39GPrzP4xJWDZi9gnQ

博士生开源深度学习C++库DLL：快速构建卷积受限玻尔兹曼机

https://mp.weixin.qq.com/s/Z1_ujG7iP_AFUD0gVO8HXQ

信息熵、交叉熵和相对熵

http://blog.csdn.net/shijing_0214/article/details/51116046

理解最大熵模型

# 概率分布（2）

上一篇《概率分布（1）》写的意犹未尽，这里继续写。本篇主要关注$$\chi^2$$分布、t分布和F分布，也就是统计学的三大祖师爷各自的看家本领。

## $$\chi^2$$分布

设$$X_1,\dots,X_n$$是来自总体$$N(0,1)$$的样本，则称统计量

$$\chi^2=X_1^2+\dots+X_n^2\tag{1}$$

服从自由度为n的$$\chi^2$$分布（chi-squared distribution），记作$$\chi^2\sim \chi^2(n)$$。其PDF为：

$$f(x;\,n) =
\begin{cases}
  \dfrac{x^{(n/2-1)} e^{-x/2}}{2^{n/2} \Gamma\left(\frac n 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.
\end{cases}$$

## t分布

设$$X\sim N(0,1),Y\sim\chi^2(n)$$，并且X、Y独立，则称随机变量

$$t=\frac{X}{\sqrt{Y/n}}\tag{2}$$

服从自由度为n的t分布（t distribution），记作$$t\sim t(n)$$。其PDF为：

$$f(t) = \frac{\Gamma(\frac{n+1}{2})} {\sqrt{n\pi}\,\Gamma(\frac{n}{2})} \left(1+\frac{t^2}{n} \right)^{\!-\frac{n+1}{2}}$$

## F分布

设$$U\sim \chi^2(d_1),V\sim\chi^2(d_2)$$，并且U、V独立，则称随机变量

$$F=\frac{U/d_1}{V/d_2}\tag{3}$$

服从自由度为$$(d_1,d_2)$$的F分布（F distribution），记作$$F\sim F(d_1,d_2)$$。其PDF为：

$$\begin{align}
f(x; d_1,d_2) &= \frac{\sqrt{\frac{(d_1\,x)^{d_1}\,\,d_2^{d_2}} {(d_1\,x+d_2)^{d_1+d_2}}}} {x\,\mathrm{B}\!\left(\frac{d_1}{2},\frac{d_2}{2}\right)} \\
&=\frac{1}{\mathrm{B}\!\left(\frac{d_1}{2},\frac{d_2}{2}\right)} \left(\frac{d_1}{d_2}\right)^{\frac{d_1}{2}} x^{\frac{d_1}{2} - 1} \left(1+\frac{d_1}{d_2}\,x\right)^{-\frac{d_1+d_2}{2}}
\end{align}$$

显然：

$$\frac{1}{F}\sim F(d_2,d_1)$$

参考：

https://mp.weixin.qq.com/s/X0nSqiQoJpEur7PM1tM-0A

每个数据科学专家都应该知道的六个概率分布

## 假设检验

假设检验就是根据样本对所提出的假设$$H_0$$作判断。

如果$$P\{拒绝H_0\vert H_0为真\}\le \alpha$$，则接受$$H_0$$。

这里的$$\alpha$$被称作**显著性水平**。假设检验$$H_0$$所涉及的统计量被称作**检验统计量**。

下表是正态总体均值、方差的检验法表格：

| $$H_0$$ | 检验统计量 | $$H_0$$为真时的统计量分布 |
|:--:|:--:|:--:|
| $$\mu=\mu_0(\sigma^2已知)$$ | $$z=\frac{\overline x-\mu_0}{\sigma/\sqrt n}$$ | $$N(0,1)$$ |
| $$\mu=\mu_0(\sigma^2未知)$$ | $$t=\frac{\overline x-\mu_0}{s/\sqrt n}$$ | $$t(n-1)$$ |
| $$\mu_1-\mu_2=\delta(\sigma_1^2,\sigma_2^2已知)$$ | $$Z=\frac{\overline x-\overline y-\delta}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}$$ | $$N(0,1)$$ |
| $$\mu_1-\mu_2=\delta(\sigma_1^2=\sigma_2^2=\sigma^2未知)$$ | $$t=\frac{\overline x-\overline y-\delta}{s_w\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},s_w^2=\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}$$ | $$t(n_1+n_2-2)$$ |
| $$\sigma^2=\sigma_0^2(\mu未知)$$ | $$\chi^2=\frac{(n-1)s^2}{\sigma_0^2}$$ | $$\chi^2(n-1)$$ |
| $$\sigma_1^2=\sigma_2^2(\mu_1,\mu_2未知)$$ | $$F=\frac{s_1^2}{s_2^2}$$ | $$F(n_1-1,n_2-1)$$ |
| $$\mu_d=0(成对数据)$$ | $$t=\frac{\overline d-0}{s/\sqrt n}$$ | $$t(n-1)$$ |

上面这些和$$\chi^2$$分布、t分布、F分布有关的假设检验，又被称作**$$\chi^2$$检验、t检验和F检验**。对均值的假设检验，被称为$$\mu$$检验。

上面这些都是正态样本的参数检验。

对于非参数检验或者非正态样本检验，其他的检验方法还有Wilcoxon signed-rank test、Kruskal–Wallis test、Friedman test等。

>注：Frank Wilcoxon，1892～1965，美国化学家。康奈尔大学博士。先后供职于几家美国化工企业的研究机构。

>William Henry ("Bill") Kruskal，1919～2005，美国数学家。哥伦比亚大学博士，芝加哥大学教授。

>Milton Friedman，1912～2006，美国经济学家。哥伦比亚大学博士，芝加哥大学教授。1976年获诺贝尔经济学奖。芝加哥学派第二代的领军人物。

>Wilson Allen Wallis，1912～1998，美国经济学家。先后就读于明尼苏达大学和芝加哥大学，但是没有博士学位。罗彻斯特大学校长。从艾森豪威尔到里根的历届共和党总统的顾问。Milton Friedman的至交。其父Wilson Dallam Wallis为美国人类学家，明尼苏达大学教授。

## 一元线性回归的显著性检验

假设y关于x的回归具有形式$$a+bx$$，则$$H_0:b=0$$。

这里使用t检验法进行假设检验。

首先，不加证明的给出如下结论：

**推论1**：$$\overline y\sim N(a+b\overline x,\sigma^2/n)$$

**推论2**：$$\hat b\sim N(b,\sigma^2/S_{xx})$$

**推论3**：$$\hat y_0=\hat a+\hat b x_0=\overline y+\hat b(x_0-\overline x)\sim N\left(a+bx_0,\left[\frac{1}{n}+\frac{(x_0-\overline x)^2}{S_{xx}}\right]\sigma^2\right)$$

**推论4**：$$Q_e/\sigma^2\sim \chi^2(n-2)$$

**推论5**：$$\overline y,\hat b,Q_e$$相互独立。

**推论6**：若$$y_0=a+bx_0+\epsilon_0$$与$$y_1,\dots,y_n$$独立，则$$y_0,\hat y_0,Q_e$$相互独立。

其中，$$\overline y$$表示y的均值，而$$\hat y$$表示y的估计值,$$S_{xx}$$表示方差，$$Q_e$$为残差平方和$$\sum_{i=1}^n(y_i-\hat y_i)^2$$。

由推论4可得：

$$E(Q_e/\sigma^2)=n-2$$

即：

$$Q_e=\hat\sigma^2(n-2)\tag{4}$$

由推论2和5、公式2和4，可得：

$$\frac{\hat b-b}{\sqrt{\sigma^2/S_{xx}}}\bigg /\sqrt{\frac{(n-2)\hat \sigma^2}{\sigma^2}\bigg /(n-2)}\sim t(n-2)$$

即：

$$\frac{\hat b-b}{\hat \sigma}\sqrt{S_{xx}}\sim t(n-2)$$

当假设$$H_0$$被拒绝时，认为回归效果是显著的，反之就认为回归效果不显著。

不显著的原因可能有以下几种：

1.影响y取值的，除了x，还有其他不可忽略因素。

2.y与x的关系不是线性的，存在其他的关系。

3.y与x不存在关系。

## KS检验

Kolmogorov–Smirnov test用于对样本是否属于某种分布进行假设检验。

>注：Andrey Nikolaevich Kolmogorov，1903～1987，二十世纪俄国最伟大的数学家之一。莫斯科州立大学博士和教授。俄罗斯科学院院士，挪威科学院和英国皇家学会外籍院士。沃尔夫奖获得者（1980年）。他在数学的许多领域都有重要贡献，以他的名字命名的理论竟有30项之多。

>由于Nobel Prizes没有数学奖，因此数学界的最高奖一般有三个：   
>1.**Fields Medal**。获奖难度最高，因为有40岁的年龄限制。在国内比较知名的丘成桐、陶哲轩都是该奖的获奖者。   
>不过他们还不是最屌的。Grigori Perelman（Poincaré conjecture的证明者）直接拒绝了Fields Medal。除此之外，他还拒绝了EMS Prize和Millennium Prize，其中后者奖金高达100万美元，而且还不知道下一个获奖者什么时候诞生（该奖不是年度奖，而是数学难题奖，数学难题的解决周期，你懂的）。   
>Perelman犹如一个特立独行的隐士，谁的账都不买，包括名利。他将他的伟大证明随手扔进arXiv这样一个非正规网站，但却被《Science》评为年度科学突破。数学界已经很多年没有这样的荣誉了。   
>补充一下，Perelman就读的中学是Kolmogorov创建的。   
>2.**Abel Prize**。和Nobel Prizes的规则相同，由于不限年龄，水平是最高的。缺点是这个奖是2001年才创建的，影响力略差。   
>3.**Wolf Prize**。在Abel Prize创建之前，被誉为数学界的Nobel Prizes。

>Nikolai Vasilyevich Smirnov，1900～1966，俄国数学家。莫斯科大学博士，斯塔克罗夫数学研究所研究员。


