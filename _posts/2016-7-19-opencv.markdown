---
layout: post
title:  OpenCV
category: technology 
---

# OpenCV

## 参考资料

OpenCV是一套跨平台计算机视觉库。其官网为：

http://opencv.org/

安装方法：

`sudo apt-get install libopencv-dev`

代码下载地址：

https://github.com/Itseez/opencv

OpenCV项目目前由itseez团队维护，他们的网站是：

http://itseez.com/

官方教程：

http://docs.opencv.org/2.4/doc/tutorials/tutorials.html

其他教程：

http://download.csdn.net/detail/antkillerfarm/9578482

《Learning OpenCV》，Gary Bradski和Adrian Kaehler合著的书籍。

Gary Rost Bradski，UC Berkeley的本科+波士顿大学博士，斯坦福大学顾问教授。OpenCV项目创始人，有20年以上的相关经验。百度首席科学家吴恩达算是他的小弟吧。

Adrian Kaehler，哥伦比亚大学博士。

https://www.coursera.org/learn/machine-learning

《斯坦福大学机器学习课程讲义》

Andrew Ng，也就是吴恩达写的讲义，写的非常浅显易懂。

http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning

Andrew Ng的公开课视频。

http://wiki.opencv.org.cn/

国人办的OpenCV中文网。

http://blog.csdn.net/morewindows/article/category/1291764

国人写的OpenCV入门指南。

http://blog.csdn.net/column/details/opencv-tutorial.html

另一个国人写的OpenCV入门指南。

http://blog.csdn.net/abcjennifer

一个浙大妹子的blog，关注计算机视觉、机器学习。现在百度深度学习实验室。

http://blog.csdn.net/xiaowei_cqu

另一个妹子的blog，也是计算机视觉方面的。重庆大学本科+北大硕士，现在Google北京。

http://blog.csdn.net/jinshengtao

一个计算机视觉、机器学习的blog。

http://www.cnblogs.com/jerrylead/

一个机器学习的blog。

http://blog.csdn.net/mmz_xiaokong/article/details/7916163

机器视觉开源处理库汇总。

http://blog.csdn.net/mmz_xiaokong/article/details/7916189

介绍n款计算机视觉库/人脸识别开源库/软件。

http://deeplearning.net/

一个深度学习方面的资料网站。从该网站提供的招聘信息来看，caffe、Theano、Torch是目前主流的三大框架库。

http://caffe.berkeleyvision.org/

caffe是贾扬清写的一个深度学习框架。这哥们是清华的本硕+UCB的博士。

https://github.com/BVLC/caffe

caffe的代码地址。

http://deeplearning.net/software/theano/

Theano的主页

https://github.com/Theano/Theano

Theano的代码地址。

http://www.scratchapixel.com/

一个学习图像处理的网站。

http://www.52nlp.cn/

一个国内的自然语言处理的网站。

http://www.cs.rug.nl/~imaging/

一个在线的图像处理网站。

## 使用细节

### saturate_cast

saturate_cast宏会对结果进行转换，以确保它在有效范围之内。

### 硬件加速

OpenCV中的运算，除了软件实现之外，还有若干种硬件加速，包括OpenCL、CUDA和IPPICV。

Intel针对自身的硬件加速，推出了IPP（Integrated Performance Primitives）软件包，但这个包是收费的。从OpenCV 3.0开始，Intel将IPP的一个子集提取出来，免费供OpenCV项目使用。这个子集，俗称“IPPICV”。

# GNU Octave

GNU Octave是Matlab的一个开源实现。它拥有和后者兼容的语法，类似的IDE，并实现了大部分的基础库。

官网：

https://gnu.org/software/octave/

安装方法:

`sudo apt-get install octave`

# R

R语言是另一个应用较广的数学工具语言和环境，主要应用于统计学和数据挖掘等领域。在这些领域，其影响力甚至超过了Matlab。

R语言的语法来自于1970年代贝尔实验室发明的S语言，因此又被称为“GNU S”。

官网：

https://www.r-project.org/

软件仓库：

https://cran.r-project.org/

安装方法:

`sudo apt-get install r-base r-base-dev`

R语言有个和python类似的命令行交互环境。除此之外，还有第三方提供的GUI环境，最常用的是RStudio，其官网：

https://www.rstudio.com/

参考：

http://cos.name/

统计之都（Capital of Statistics，简称COS）成立于2006年5月，是一个旨在推广与应用统计学知识的网站和社区。

http://yihui.name/

谢益辉，Iowa State University博士，目前就职于RStudio。中国R语言会议、COS社区创办者。

# K-Means算法

http://www.csdn.net/article/2012-07-03/2807073-k-means

http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html

http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html

# Learning Theory

# Regularization and model selection

# Mixtures of Gaussians and the EM algorithm

# The EM algorithm

# Factor analysis

# Principal components analysis

# Independent Components Analysis

# Reinforcement Learning and Control

## 互不相容事件、独立事件与条件独立事件

如果$$P(AB)=0$$,则事件A、B为互不相容事件。

如果$$P(AB)=P(A)P(B)或P(A)=P(A\lvert B)$$,则事件A、B为独立事件。

如果$$P(AB\lvert R)=P(A\lvert R)P(B\lvert R)$$,则事件A、B为条件R下的独立事件。

可见，这三者是完全不同的数学概念，不要搞混了。

## 朴素贝叶斯方法

这里以文本分析（Text Classification）为例，讲解一下朴素贝叶斯（Naive Bayes）方法。

文本分析的一个常用方法是根据词典建立特征向量x。x中的每个元素代表词典里的一个单词，如果该单词在文本中出现，则对应的元素值为1,否则为0。

这里假设我们的目标是通过文本分析，判别一个email是否是垃圾邮件。$$y=1$$表示是垃圾邮件，$$y=0$$表示不是垃圾邮件。显然，在这里一封邮件就是一个样本集。

相比之前的应用领域，文本分析的特殊之处在于词典中的单词数量十分庞大，从而导致x的维数巨大（比如50000个单词，就是50000维），以至于之前的方法，根本无法计算。

为了简化问题，我们假设$$p(x_i\lvert y)$$是条件独立的。这个假设被称为朴素贝叶斯假设（Naive Bayes (NB) assumption）。使用这个假设的算法被称为朴素贝叶斯分类器（Naive Bayes classifier）。

从数学角度，NB假设是个很严格的条件，但是实际使用中，即使样本集不满足NB假设，使用NB方法的效果一般还是不错的。

$$\begin{align}p(x_1,\dots,x_{50000}\lvert y)&=p(x_1\lvert y)p(x_2\lvert y,x_1)p(x_3\lvert y,x_1,x_2)\cdots p(x_{50000}\lvert y,x_1,\dots,x_{49999})(条件概率的乘法公式)
\\&=p(x_1\lvert y)p(x_2\lvert y)p(x_3\lvert y)\cdots p(x_{50000}\lvert y)(NB假设)=\prod_{i=1}^np(x_i\lvert y)
\end{align}$$

因此：

$$\begin{align}p(y=1\lvert x)&=\frac{p(x\lvert y=1)p(y=1)}{p(x)}
\\&=\frac{(\prod_{i=1}^np(x_i\lvert y=1))p(y=1)}{(\prod_{i=1}^np(x_i\lvert y=1))p(y=1)+(\prod_{i=1}^np(x_i\lvert y=0))p(y=0)}（公式1）
\end{align}$$

最大似然估计值为：

$$\phi_{j\lvert y=1}=p(x_j=1\lvert y=1)=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\land y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}$$

$$\phi_{j\lvert y=0}=p(x_j=1\lvert y=0)=\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\land y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}$$

$$\phi_y=p(y=1)=\frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}$$

NB算法还可以扩展到y有多个取值的情况。对于y为连续函数的情况，可以采用区间离散化操作，将之离散化为多个离散值。

## 拉普拉斯平滑

对于样本集中未出现的单词，在其首次出现时，由于先验概率$$p(x_i\lvert y=1)=0,p(x_i\lvert y=0)=0$$，这时公式1会出现$$\frac{0}{0}$$的情况。

为了避免这种情况，我们假定先验概率至少为1次，也就是

$$\phi_j=p(y=j)=\frac{\sum_{i=1}^m1\{y^{(i)}=j\}+1}{m+k}$$

这种方法叫做拉普拉斯平滑（Laplace Smoothing）。注意这里$$\phi$$的定义和上面略有不同，上面的公式中，y是二值分布，而这里是多值分布（值为k）。为了满足概率和为1的条件，分母上需要加k。

## 文本分类的事件模型

文本分类的事件模型有两类：

1.多值伯努利事件模型（multi-variate Bernoulli event model）。

在这个模型中，我们首先随机选定了邮件的类型（垃圾或者普通邮件，也就是$$p(y)$$），然后翻阅词典，随机决定一个词是否要在邮件中出现，出现则$$x_i$$标示为1，否则标示为0。然后将出现的词，组成一封邮件。一个词是否出现的概率为$$p(x_i\lvert y)$$，整封邮件的概率为$$p(y)\prod_{i=1}^np(x_i\lvert y)$$。

2.多项事件模型（multinomial event model）。

令$$x_i$$表示邮件的第i个词在字典中的位置，那么$$x_i$$的取值范围为$${1,2,...\lvert V\rvert}$$，$$\lvert V\rvert$$表示字典中单词的数目。这样一封邮件可以表示成$$(x_1,x_2,\dots,x_n)$$，这里n为邮件包含的单词个数，显然每个邮件的n值一般是不同的。

这相当于重复投掷$$\lvert V\rvert$$面的骰子，将观察值记录下来就形成了一封邮件。每个面的概率服从$$p(x_i\lvert y)$$，而且每次试验条件独立。这样我们得到的邮件概率是$$p(y)\prod_{i=1}^np(x_i\lvert y)$$。

需要注意的是，上面两个事件模型的概率公式虽然一致，但含义却有很大差异，不要弄混了。

# 支持向量机 Support Vector Machines

支持向量机是目前最好的监督学习算法。

我们首先来看一幅图：

![](/images/article/SVM.png)

上图中的x和o分别代表y的两种不同取值。前面提到线性回归的预测函数为：

$$h_\theta(x)=g(\theta^Tx)$$




