---
layout: post
title:  OpenCV
category: technology 
---

# OpenCV

## 参考资料

OpenCV是一套跨平台计算机视觉库。其官网为：

http://opencv.org/

安装方法：

`sudo apt-get install libopencv-dev`

代码下载地址：

https://github.com/Itseez/opencv

OpenCV项目目前由itseez团队维护，他们的网站是：

http://itseez.com/

官方教程：

http://docs.opencv.org/2.4/doc/tutorials/tutorials.html

其他教程：

http://download.csdn.net/detail/antkillerfarm/9578482

《Learning OpenCV》，Gary Bradski和Adrian Kaehler合著的书籍。

Gary Rost Bradski，UC Berkeley的本科+波士顿大学博士，斯坦福大学顾问教授。OpenCV项目创始人，有20年以上的相关经验。百度首席科学家吴恩达算是他的小弟吧。

Adrian Kaehler，哥伦比亚大学博士。

https://www.coursera.org/learn/machine-learning

《斯坦福大学机器学习课程讲义》

Andrew Ng，也就是吴恩达写的讲义，写的非常浅显易懂。

http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning

Andrew Ng的公开课视频。

http://wiki.opencv.org.cn/

国人办的OpenCV中文网。

http://blog.csdn.net/morewindows/article/category/1291764

国人写的OpenCV入门指南。

http://blog.csdn.net/column/details/opencv-tutorial.html

另一个国人写的OpenCV入门指南。

http://blog.csdn.net/abcjennifer

一个浙大妹子的blog，关注计算机视觉、机器学习。现在百度深度学习实验室。

http://blog.csdn.net/xiaowei_cqu

另一个妹子的blog，也是计算机视觉方面的。重庆大学本科+北大硕士，现在Google北京。

http://blog.csdn.net/jinshengtao

一个计算机视觉、机器学习的blog。

http://www.cnblogs.com/jerrylead/

一个机器学习的blog。

http://blog.csdn.net/mmz_xiaokong/article/details/7916163

机器视觉开源处理库汇总。

http://blog.csdn.net/mmz_xiaokong/article/details/7916189

介绍n款计算机视觉库/人脸识别开源库/软件。

http://deeplearning.net/

一个深度学习方面的资料网站。从该网站提供的招聘信息来看，caffe、Theano、Torch是目前主流的三大框架库。

http://caffe.berkeleyvision.org/

caffe是贾扬清写的一个深度学习框架。这哥们是清华的本硕+UCB的博士。

https://github.com/BVLC/caffe

caffe的代码地址。

http://deeplearning.net/software/theano/

Theano的主页

https://github.com/Theano/Theano

Theano的代码地址。

http://www.scratchapixel.com/

一个学习图像处理的网站。

http://www.52nlp.cn/

一个国内的自然语言处理的网站。

http://www.cs.rug.nl/~imaging/

一个在线的图像处理网站。

## 使用细节

### saturate_cast

saturate_cast宏会对结果进行转换，以确保它在有效范围之内。

### 硬件加速

OpenCV中的运算，除了软件实现之外，还有若干种硬件加速，包括OpenCL、CUDA和IPPICV。

Intel针对自身的硬件加速，推出了IPP（Integrated Performance Primitives）软件包，但这个包是收费的。从OpenCV 3.0开始，Intel将IPP的一个子集提取出来，免费供OpenCV项目使用。这个子集，俗称“IPPICV”。

# GNU Octave

GNU Octave是Matlab的一个开源实现。它拥有和后者兼容的语法，类似的IDE，并实现了大部分的基础库。

官网：

https://gnu.org/software/octave/

安装方法:

`sudo apt-get install octave`

# R

R语言是另一个应用较广的数学工具语言和环境，主要应用于统计学和数据挖掘等领域。在这些领域，其影响力甚至超过了Matlab。

R语言的语法来自于1970年代贝尔实验室发明的S语言，因此又被称为“GNU S”。

官网：

https://www.r-project.org/

软件仓库：

https://cran.r-project.org/

安装方法:

`sudo apt-get install r-base r-base-dev`

R语言有个和python类似的命令行交互环境。除此之外，还有第三方提供的GUI环境，最常用的是RStudio，其官网：

https://www.rstudio.com/

参考：

http://cos.name/

统计之都（Capital of Statistics，简称COS）成立于2006年5月，是一个旨在推广与应用统计学知识的网站和社区。

http://yihui.name/

谢益辉，Iowa State University博士，目前就职于RStudio。中国R语言会议、COS社区创办者。

# 线性回归

线性回归属于有监督学习（supervised learning）的其中一种方法。

对于一个给定的训练集（training set）$$(x,y)$$，其中$$y=h(x)=h(x_1,x_2,...,x_n)$$，在$$h$$未知的情况下，求得$$h$$或者$$h$$的近似解的过程，被称为猜测（hypothesis）。hypothesis的目的是，能在给定$$x$$的情况下，预测$$y$$。

如果$$y$$是连续函数，那么这个过程叫做回归（regression）问题。如果$$y$$是离散函数，那么这个过程叫做分类（classification）问题。

$$h_{\theta}(x)=\theta_0+\theta_1x_1+...+\theta_nx_n=\sum_{i=0}^n\theta_ix_i=\theta^Tx（公式1）$$

满足公式1条件的回归问题，被称作线性回归（Linear Regression）。

为了评估公式1中待定系数$$\theta$$的预测准确度，我们定义如下代价函数（cost function）：

$$J(\theta)=\frac{1}{2}\sum_{i=0}^m(h_{\theta}(x^{(i)})-y^{(i)})^2（公式2）$$

其中，m表示训练集的个数（从0算起），$$x^{(i)}$$表示第i个训练样本。

代价函数的表达式，和正态分布的最大似然估计有关，这实际上就是正态分布的方差计算公式。显然，代价函数的值越小，预测准确度越高。

## Jacobian矩阵

Jacobian矩阵是矩阵A的一阶导数矩阵，定义如下：

$$J(A)=\frac{\mathrm{d}f}{\mathrm{d}x}=
\begin{bmatrix}
     \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
     \vdots & \ddots & \vdots \\
     \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}    
\end{bmatrix}
$$

当$$m=1$$时，该矩阵又被称为梯度向量：

$$\nabla(A)=\begin{bmatrix}
     \frac{\partial f}{\partial x_1} & \cdots & \frac{\partial f}{\partial x_n} \\ 
\end{bmatrix}$$

## Hessian矩阵

Hessian矩阵是矩阵A的二阶导数矩阵，定义如下：

$$H(A)=
\begin{bmatrix}
     \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n} \\
     \frac{\partial^2 f}{\partial x_2\partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2\partial x_n} \\
     \vdots & \vdots & \ddots & \vdots \\
     \frac{\partial^2 f}{\partial x_n\partial x_1} & \frac{\partial^2 f}{\partial x_n\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \\    
\end{bmatrix}
$$

因为$$\frac{\partial^2 f}{\partial x_i\partial x_j}=\frac{\partial^2 f}{\partial x_j\partial x_i}$$(克莱罗定理，Clairaut’s theorem)，所以Hessian矩阵通常是一个对称矩阵。

参考：

http://blog.csdn.net/dsbatigol/article/details/12558891

## 梯度下降算法

公式2的求极值问题，实际上就是求驻点（stationary point）的问题，即求$$\nabla(f)=0$$的点的问题。

梯度下降（gradient descent）算法的原理，从直观来说，就是沿着梯度向量的反方向，向坡底前进。该算法是一个递归算法，其迭代公式为：

$$x_{k+1}=x_k-t_kA_k\nabla f(x_k)$$

其中，$$t_k$$表示迭代的步长，$$\nabla f(x_k)$$为梯度向量，$$A_k$$为系数矩阵。根据$$t_k$$和$$A_k$$的取法不同会产生出各种各样的变种算法。

例如，当$$A_k=H(x_k)^{-1}$$时，就是著名的牛顿迭代法（Newton Method）。可以证明，牛顿迭代法具有二阶收敛性（在二阶以内的所有系数矩阵中，收敛最快）。

名称 | 梯度下降算法 | 牛顿迭代法
|:--:|:--:|:--:|
迭代次数 | 收敛慢，迭代次数多。 | 收敛快，迭代次数少。
迭代计算复杂度 | $$O(n)$$ | $$O(n^3)$$
实现难点 | 步长需要一定的方法来确定。 | $$H(x_k)$$需要满秩，且$$H(x_k)^{-1}$$的计算比较复杂，尤其是维数很高的时候。

梯度下降算法只有当函数为凸函数时，才会严格收敛到最小值。否则的话，可能只是极小值，而非最小值。如下图所示：

![](/images/article/gradient_descent.png)

参考：

http://freemind.pluskid.org/machine-learning/gradient-descent-wolfe-s-condition-and-logistic-regression/

http://freemind.pluskid.org/machine-learning/newton-method/

## LMS算法

LMS（least mean squares，最小均方）算法，也是一种迭代算法。其迭代公式为：

$$\theta_j:=\theta_j-\alpha\frac{\partial J(\theta)}{\partial \theta_j}（公式3）$$

其中，$$:=$$表示赋值操作，$$J(\theta)$$是公式2所示的代价函数，$$\alpha$$被称作学习率（learning rate）。

可以看出，LMS的迭代公式是梯度下降算法的一个特例，其中，$$\alpha$$相当于步长，系数矩阵$$A=E$$，这里的$$E$$表示单位矩阵。

将公式2代入公式3，可得：

$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_j（公式4）$$

迭代方式分为两种：

1.批量梯度下降（batch gradient descent）算法。方法如下：

>Repeat until convergence {<br/>
><span style="white-space: pre">	</span>
>$$\theta_j:=\theta_j+\alpha\sum_{i=1}^m(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_j$$(for every j)
><br/>}

2.随机梯度下降（stochastic gradient descent）算法。方法如下：

>Loop {<br/>
><span style="white-space: pre">	</span>for i=1 to m, {<br/>
><span style="white-space: pre">	</span><span style="white-space: pre">	</span>
>$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_j$$(for every j)
><br/><span style="white-space: pre">	</span>}
><br/>}

名称 | 批量梯度下降 | 随机梯度下降
|:--:|:--:|:--:|
特点 | 每向前走一步，都需要遍历整个训练样本集。 | 一个样本接着一个样本的处理数据。
计算复杂度 | 大 | 小
收敛程度 | 收敛 | 由于新样本引入新的误差，该算法只能收敛到一定程度，而不能无限逼近最小值。

## 正规方程组算法

正规方程组（Normal Equations）算法，是传统的以解方程的方式求最小值的方法。

如果，令

$$X=\begin{bmatrix}
     (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(m)})^T
\end{bmatrix},
\vec{y}=\begin{bmatrix}
     (y^{(1)}) \\ (y^{(2)}) \\ \vdots \\ (y^{(m)})
\end{bmatrix}$$

则：

$$\theta=(X^TX)^{-1}X^T\vec{y}$$

这种解方程的算法，实际上就是通常所说的最小二乘法（Least squares）。

优点：解是精确解，而不是近似解。不是迭代算法，程序实现简单。不在意$$X$$特征的scale。比如，特征向量$$X=\{x_1, x_2\}$$, 其中$$x_1$$的range为1~2000，而$$x_2$$的range为1~4，可以看到它们的范围相差了500倍。如果使用梯度下降方法的话，会导致椭圆变得很窄很长，而出现梯度下降困难，甚至无法下降梯度（因为导数乘上步长后可能会冲到椭圆的外面）的问题。

缺点：维数高的时候，矩阵求逆运算的计算量很大。



# K-Means算法

http://www.csdn.net/article/2012-07-03/2807073-k-means

http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html

http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html

