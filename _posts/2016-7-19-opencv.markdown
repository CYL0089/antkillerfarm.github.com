---
layout: post
title:  OpenCV
category: technology 
---

# OpenCV

## 参考资料

OpenCV是一套跨平台计算机视觉库。其官网为：

http://opencv.org/

安装方法：

`sudo apt-get install libopencv-dev`

代码下载地址：

https://github.com/Itseez/opencv

OpenCV项目目前由itseez团队维护，他们的网站是：

http://itseez.com/

官方教程：

http://docs.opencv.org/2.4/doc/tutorials/tutorials.html

其他教程：

http://download.csdn.net/detail/antkillerfarm/9578482

《Learning OpenCV》，Gary Bradski和Adrian Kaehler合著的书籍。

Gary Rost Bradski，UC Berkeley的本科+波士顿大学博士，斯坦福大学顾问教授。OpenCV项目创始人，有20年以上的相关经验。百度首席科学家吴恩达算是他的小弟吧。

Adrian Kaehler，哥伦比亚大学博士。

https://www.coursera.org/learn/machine-learning

《斯坦福大学机器学习课程讲义》

Andrew Ng，也就是吴恩达写的讲义，写的非常浅显易懂。

http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning

Andrew Ng的公开课视频。

http://wiki.opencv.org.cn/

国人办的OpenCV中文网。

http://blog.csdn.net/morewindows/article/category/1291764

国人写的OpenCV入门指南。

http://blog.csdn.net/column/details/opencv-tutorial.html

另一个国人写的OpenCV入门指南。

http://blog.csdn.net/abcjennifer

一个浙大妹子的blog，关注计算机视觉、机器学习。现在百度深度学习实验室。

http://blog.csdn.net/xiaowei_cqu

另一个妹子的blog，也是计算机视觉方面的。重庆大学本科+北大硕士，现在Google北京。

http://blog.csdn.net/jinshengtao

一个计算机视觉、机器学习的blog。

http://www.cnblogs.com/jerrylead/

一个机器学习的blog。

http://blog.csdn.net/mmz_xiaokong/article/details/7916163

机器视觉开源处理库汇总。

http://blog.csdn.net/mmz_xiaokong/article/details/7916189

介绍n款计算机视觉库/人脸识别开源库/软件。

http://deeplearning.net/

一个深度学习方面的资料网站。从该网站提供的招聘信息来看，caffe、Theano、Torch是目前主流的三大框架库。

http://caffe.berkeleyvision.org/

caffe是贾扬清写的一个深度学习框架。这哥们是清华的本硕+UCB的博士。

https://github.com/BVLC/caffe

caffe的代码地址。

http://deeplearning.net/software/theano/

Theano的主页

https://github.com/Theano/Theano

Theano的代码地址。

http://www.scratchapixel.com/

一个学习图像处理的网站。

http://www.52nlp.cn/

一个国内的自然语言处理的网站。

http://www.cs.rug.nl/~imaging/

一个在线的图像处理网站。

## 使用细节

### saturate_cast

saturate_cast宏会对结果进行转换，以确保它在有效范围之内。

### 硬件加速

OpenCV中的运算，除了软件实现之外，还有若干种硬件加速，包括OpenCL、CUDA和IPPICV。

Intel针对自身的硬件加速，推出了IPP（Integrated Performance Primitives）软件包，但这个包是收费的。从OpenCV 3.0开始，Intel将IPP的一个子集提取出来，免费供OpenCV项目使用。这个子集，俗称“IPPICV”。

# GNU Octave

GNU Octave是Matlab的一个开源实现。它拥有和后者兼容的语法，类似的IDE，并实现了大部分的基础库。

官网：

https://gnu.org/software/octave/

安装方法:

`sudo apt-get install octave`

# R

R语言是另一个应用较广的数学工具语言和环境，主要应用于统计学和数据挖掘等领域。在这些领域，其影响力甚至超过了Matlab。

R语言的语法来自于1970年代贝尔实验室发明的S语言，因此又被称为“GNU S”。

官网：

https://www.r-project.org/

软件仓库：

https://cran.r-project.org/

安装方法:

`sudo apt-get install r-base r-base-dev`

R语言有个和python类似的命令行交互环境。除此之外，还有第三方提供的GUI环境，最常用的是RStudio，其官网：

https://www.rstudio.com/

参考：

http://cos.name/

统计之都（Capital of Statistics，简称COS）成立于2006年5月，是一个旨在推广与应用统计学知识的网站和社区。

http://yihui.name/

谢益辉，Iowa State University博士，目前就职于RStudio。中国R语言会议、COS社区创办者。

# K-Means算法

http://www.csdn.net/article/2012-07-03/2807073-k-means

http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html

http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html

# Support Vector Machines

# Learning Theory

# Regularization and model selection

# Mixtures of Gaussians and the EM algorithm

# The EM algorithm

# Factor analysis

# Principal components analysis

# Independent Components Analysis

# Reinforcement Learning and Control

## 指数类分布

线性回归和对数回归的迭代公式相同不是偶然的，它们都是指数类分布的特例。

指数类分布（exponential family distributions）的标准形式如下：

$$p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))$$

其中，$$\eta$$被称作自然参数（natural parameter）或正准参数（canonical parameter），$$T(y)$$被称作充分统计量（sufficient statistic）。

$$a(\eta)$$是对数配分函数（log partition function），它存在的目的是利用$$e^{-a(\eta)}$$进行约束，以使:

$$\sum_Y p(y;\eta)=1或\int_Y p(y;\eta)=1$$

伯努利分布到指数类分布的变换过程如下：

$$\begin{align}p(y:\phi)&=\phi^y(1-\phi)^{1-y}=\exp(\log(\phi^y(1-\phi)^{1-y}))
\\&=\exp(y\log(\phi)+(1-y)\log(1-\phi))
\\&=\exp(y\log(\frac{\phi}{1-\phi})+\log(1-\phi))
\end{align}$$

可见：

$$\begin{align}& b(y)=1 \\& \eta=\log(\frac{\phi}{1-\phi})\Rightarrow \phi=\frac{1}{1+e^{-\eta}} \\& T(y)=y \\& a(\eta)=-\log(1-\phi) \\\end{align}$$

高斯分布到指数类分布的变换过程如下：

$$\begin{align}p(y;\mu)&=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(y-\mu)^2\right)
\\&=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y^2\right)\cdot\exp\left(\mu y-\frac{1}{2}\mu^2\right)\dot\
\end{align}$$

可见：

$$\begin{align}& \eta=\mu \\& T(y)=y \\& a(\eta)=\frac{\mu^2}{2} \\& b(y)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y^2\right) \\\end{align}$$

除此之外，Dirichlet分布、Poisson分布、多项分布、$$\beta$$分布、$$\gamma$$分布都是指数类分布。

## 广义线性模型

广义线性模型（Generalized Linear Model，GLM）是解决指数类分布的回归问题的通用模型。它基于以下三个假设：

$$y\mid x;\theta \sim ExponentialFamily(\eta)(公式1)$$

$$h(x)=E[T(y)\mid x](公式2)$$

$$\eta=\theta^Tx(公式3)$$

下面以多项分布为例展示一下GLM的处理方法。

$$y$$的取值为$$k$$个离散值的分布，被称为$$k$$项分布。显然$$k=2$$时，就是二项分布了。

这里将$$k$$个离散值出现的概率记作$$\phi_1,...,\phi_k$$。由于$$\sum_{i=1}^k=1$$，因此，$$k$$项分布的自由度为$$k-1$$。

定义$$k-1$$维空间上的向量$$T(y)$$：

$$T(1)=\begin{bmatrix} 1 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix},
T(2)=\begin{bmatrix} 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix},
T(k-1)=\begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix},
T(k)=\begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
$$

我们使用$$(T(y))_i$$表示$$T(y)$$的第$$i$$个元素。

定义函数$$1\{True\}=1,1\{False\}=0$$，则$$(T(y))_i=1\{y=i\},E[(T(y))_i]=P(y=i)=\phi_i$$。

$$\begin{align}p(y:\phi)&=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots \phi_k^{1\{y=k\}}=\phi_1^{1\{y=1\}}\phi_2^{1\{y=2\}}\cdots \phi_k^{1-\sum_{i=1}^{k-1}1\{y=i\}}
\\&=\phi_1^{(T(y))_1}\phi_2^{(T(y))_2}\cdots \phi_k^{1-\sum_{i=1}^{k-1}(T(y))_i}
\\&=\exp((T(y))_1\log(\phi_1)+(T(y))_2\log(\phi_2)+\cdots+(1-\sum_{i=1}^{k-1}(T(y))_i)\log(\phi_k))
\\&=\exp((T(y))_1\log(\frac{\phi_1}{\phi_k})+(T(y))_2\log(\frac{\phi_2}{\phi_k})+\cdots+(T(y))_{k-1}\log(\frac{\phi_{k-1}}{\phi_k})+\log(\phi_k))
\end{align}$$

可见，

$$\eta=\begin{bmatrix} \log(\frac{\phi_1}{\phi_k}) \\ \log(\frac{\phi_2}{\phi_k}) \\ \vdots \\ \log(\frac{\phi_{k-1}}{\phi_k}) \end{bmatrix},a(\eta)=-\log(\phi_k),b(y)=1$$

$$\eta_i=\log(\frac{\phi_i}{\phi_k})(公式4)\Rightarrow e^{\eta_i}=\frac{\phi_i}{\phi_k}\Rightarrow \phi_ke^{\eta_i}=\phi_i$$

$$\Rightarrow \phi_k\sum_{i=1}^ke^{\eta_i}=\sum_{i=1}^k\phi_i=1\Rightarrow \phi_k=\frac{1}{\sum_{i=1}^ke^{\eta_i}}（公式5）$$

由公式4、5可得：

$$\phi_i=\frac{e^{\eta_i}}{\sum_{j=1}^ke^{\eta_j}}=\frac{e^{\theta_i^Tx}}{\sum_{j=1}^ke^{\theta_j^Tx}}$$

这种从$$\eta$$映射到$$\phi$$的函数，被称作softmax函数。

$$h_\theta(x)=E[T(y)\mid x;\theta]=\begin{bmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_{k-1} \end{bmatrix}
=\begin{bmatrix} \frac{\exp(\theta_1^Tx)}{\sum_{j=1}^k\exp(\theta_j^Tx)} \\ \frac{\exp(\theta_2^Tx)}{\sum_{j=1}^k\exp(\theta_j^Tx)} \\ \vdots \\ \frac{\exp(\theta_{k-1}^Tx)}{\sum_{j=1}^k\exp(\theta_j^Tx)} \end{bmatrix}$$

最大似然估计对数函数：

$$l(\theta)=\sum_{i=1}^m\log p(y^{(i)}\mid x^{(i)};\theta)=\sum_{i=1}^m\log\prod_{l=1}^k\left(\frac{\exp(\theta_{l}^Tx^{(i)})}{\sum_{j=1}^k\exp(\theta_j^Tx^{(i)})}\right)^{1\{y^{(i)}=l\}}$$

## 机器学习的优化问题

优化理论和算法是机器学习用于处理问题的重要工具，但是机器学习有自己独特的看待问题的视角，并且其中也有很多和 Optimization 并不直接相关的部分，反过来Machine Learning也对Optimization产生影响。

例如，Interior Point Method的发明被认为是Optimization中的重要里程碑，这一类的方法能够保证在多项式次迭代内收敛。但是在机器学习中，特别是现在的所谓“大数据”的趋势下，这类算法却没法工作，一方面由于机器学习中所处理的数据通常维度非常高，从而相应的优化问题的变量个数变得很巨大，传统的方法虽然保证多项式迭代收敛，但是其中每一步迭代的计算代价却是随着变量个数的平方甚至三次方增长，结果是连算法的一次迭代都无法在可接受的时间内完成。于是（机器学习方面的）人们逐渐将注意力集中到主要基于first-order oracle的单次迭代计算量非常小的算法上。另一方面，数据点的个数的爆炸性增长也使得stochastic类的算法受到更多的关注——同样是降低单次迭代的计算复杂度。

# 生成学习算法

比如说，要确定一只羊是山羊还是绵羊。从历史数据中学习到模型，然后通过提取这只羊的特征，来预测出这只羊是山羊还是绵羊。这种方法叫做判别学习算法（DLA，Discriminative Learning Algorithm）。其形式化的写法是：$$p(y\mid x)$$。

换一种思路，我们可以根据山羊的特征首先学习出一个山羊模型，然后根据绵羊的特征学习出一个绵羊模型。然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是哪个。这种方法叫做生成学习算法（GLA，Generative Learning Algorithms）。其形式化的写法是：建立模型——$$p(x\mid y)$$，应用模型——$$p(y)$$。

由贝叶斯（Bayes）公式可知：

$$p(y\mid x)=\frac{p(x\mid y)p(y)}{p(x)}(公式1)$$

其中，$$p(x\mid y)$$称为后验概率，$$p(y)$$称为先验概率。

由于我们关注的是y的离散值结果中哪个概率大（比如山羊概率和绵羊概率哪个大），而并不是关心具体的概率，因此公式1可改写为：

$$\arg\max_yp(y\mid x)=\arg\max_y\frac{p(x\mid y)p(y)}{p(x)}=\arg\max_yp(x\mid y)p(y)$$

## 高斯判别分析

高斯分布的向量形式$$N(\mu,\Sigma)$$的概率密度函数为：

$$p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}}$$

高斯判别分析(GDA，Gaussian Discriminant Analysis)

