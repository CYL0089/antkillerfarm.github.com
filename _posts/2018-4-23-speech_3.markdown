---
layout: post
title:  语音识别（三）——声学模型, 解码器技术, DTW
category: graphics 
---

# 其他前端问题（续）

## 回声抵消

严格来说，这里不应该叫回声，应该叫“自噪声”。回声是混响的延伸概念，这两者的区别就是回声的时延更长。一般来说，超过100毫秒时延的混响，人类能够明显区分出，似乎一个声音同时出现了两次，我们就叫做回声，比如天坛著名的回声壁。

实际上，这里所指的是语音交互设备自己发出的声音，比如Echo音箱，当播放歌曲的时候若叫Alexa，这时候麦克风阵列实际上采集了正在播放的音乐和用户所叫的Alexa声音，显然语音识别无法识别这两类声音。回声抵消就是要去掉其中的音乐信息而只保留用户的人声，之所以叫回声抵消，只是延续大家的习惯而已，其实是不恰当的。

## 参考

https://zhuanlan.zhihu.com/p/27977550

极限元：智能语音前端处理中的几个关键问题

https://zhuanlan.zhihu.com/p/24139910

远场语音交互中的麦克风阵列技术解读

https://zhuanlan.zhihu.com/p/22512377

自然的语音交互——麦克风阵列

# 语言模型

语言模型是针对某种语言建立的概率模型，目的是建立一个能够描述给定词序列在语言中的出现的概率的分布。

给定下边两句话：

定义机器人时代的大脑引擎，让生活更便捷、更有趣、更安全。

代时人机器定义引擎的大脑，生活让更便捷，有趣更，安更全。

语言模型会告诉你，第一句话的概率更高，更像一句”人话”。

语言模型技术广泛应用于语音识别、OCR、机器翻译、输入法等产品上。语言模型建模过程中，包括词典、语料、模型选择，对产品的性能有至关重要的影响。Ngram模型是最常用的建模技术，采用了马尔科夫假设，目前广泛地应用于工业界。

>语言模型属于NLP的范畴，这里不再赘述。

参考：

https://zhuanlan.zhihu.com/p/23504402

语言模型技术

# 声学模型

声学模型主要有两个问题，分别是特征向量序列的可变长和音频信号的丰富变化性。

**可变长特征向量序列**问题在学术上通常有动态时间规划（Dynamic Time Warping, DTW）和隐马尔科夫模型（Hidden Markov Model, HMM）方法来解决。

**音频信号的丰富变化性**是由说话人的各种复杂特性或者说话风格与语速、环境噪声、信道干扰、方言差异等因素引起的。声学模型需要足够的鲁棒性来处理以上的情况。

在过去，主流的语音识别系统通常使用梅尔倒谱系数（Mel-Frequency Cepstral Coefficient, MFCC）或者线性感知预测（Perceptual Linear Prediction, PLP）作为特征，使用混合高斯模型-隐马尔科夫模型（GMM-HMM）作为声学模型。

在近些年，区分性模型，比如深度神经网络（Deep Neural Network, DNN）在对声学特征建模上表现出更好的效果。基于深度神经网络的声学模型，比如上下文相关的深度神经网络-隐马尔科夫模型（CD-DNN-HMM）在语音识别领域已经大幅度超越了过去的GMM-HMM模型。

参考：

https://zhuanlan.zhihu.com/p/23567981

声学模型

# 解码器技术

解码器模块主要完成的工作包括：给定输入特征序列$$x_1^T$$的情况下，在由声学模型、声学上下文、发音词典和语言模型等四种知识源组成的搜索空间（Search Space）中，通过维特比（Viterbi）搜索，寻找最佳词串$$[w_1^N]^{opt}=[w_1,\dots,w_N]_{opt}$$，使得满足：

$$[w_1^N]^{opt}=\mathop{\arg\max}_{w_1^N,N}p(w_1^N\mid x_1^T)$$

在解码过程中，各种解码器的具体实现可以是不同的。按搜索空间的构成方式来分，有动态编译和静态编译两种方式。

**静态编译**，是把所有知识源统一编译在一个状态网络中，在解码过程中，根据节点间的转移权重获得概率信息。由AT&T提出的Weighted Finite State Transducer（WFST）方法是一种有效编译搜索空间并消除冗余信息的方法。

**动态编译**，预先将发音词典编译成状态网络构成搜索空间，其他知识源在解码过程中根据活跃路径上携带的历史信息动态集成。

参考：

https://zhuanlan.zhihu.com/p/23648888

语音识别之解码器技术简介

# 人类声音

成年男性：80-140 Hz

成年女性：130-220 Hz

儿童：180-320 Hz

从信号处理的角度，人类声音的处理方式和普通的雷达信号处理并无本质差异，主要的区别在于：**雷达信号经过了载波调制，而人类声音则没有这个步骤。**

参考：

https://wenku.baidu.com/view/6123ba2f0066f5335a8121fe.html

人声频率范围及各频段音色效果

# DTW

Dynamic Time Warping是Vintsiuk于1968年提出的算法。

>Taras Klymovych Vintsiuk，1939～2012，乌克兰科学家，毕业于Kyiv Polytechnic Institute。模式识别专家，语音识别领域的奠基人之一。

![](/images/img2/Dynamic_time_warping.png)

如上图所示，因为语音信号具有相当大的随机性，即使同一个人在不同时刻发同一个音，也不可能具有完全的时间长度。而且同一个单词内的不同音素的发音速度也不同，比如有的人会把“A”这个音拖得很长，或者把“i”发的很短。在这些复杂情况下，使用传统的欧几里得距离，无法有效地求得两个时间序列之间的距离（或者相似性）。

回到上面的图。如果我们将两个序列中相关联的点，用上图中的虚线连接的话，就会发现这两个序列实际上是很相似的。

那么如何用数学的方式描述上述DTW算法的思想呢？

假设现在有一个标准的参考模板R，是一个M维的向量，即$$R=\{R(1),R(2)，\dots，R(M)\}$$，每个分量可以是一个数或者是一个更小的向量。现在有一个才测试的模板T，是一个N维向量，即$$T=\{T(1),T(2)，\dots，T(N)\}$$同样每个分量可以是一个数或者是一个更小的向量，注意M不一定等于N，但是每个分量的维数应该相同。

然后，将两个序列二维展开得到下图：

![](/images/img2/DTW_2.jpg)

这样，两个序列中点与点之间的关联关系，就可以用这个二维矩阵W来表述。比如，可以用W(i,j)表示第1个序列中的第i个点和第2个序列中的第j个点相对应。所有这样的W(i,j)最终构成了上图中的曲线。这条曲线也被称作归整路径（Warp Path）。

显然，这个归整路径不是随意选择的，它需要满足以下几个约束：

1）边界条件：$$w_1=(1,1)$$和$$w_k=(m,n)$$。任何一种语音的发音快慢都有可能变化，但是其各部分的先后次序不可能改变，因此所选的路径必定是从左下角出发，在右上角结束。

2）连续性：如果$$w_{k-1}= (a’, b’)$$，那么对于路径的下一个点wk=(a, b)需要满足 (a-a’) <=1和 (b-b’) <=1。也就是不可能跨过某个点去匹配，只能和自己相邻的点对齐。这样可以保证Q和C中的每个坐标都在W中出现。

3）单调性：如果wk-1= (a’, b’)，那么对于路径的下一个点wk=(a, b)需要满足0<=(a-a’)和0<= (b-b’)。这限制W上面的点必须是随着时间单调进行的。以保证图B中的虚线不会相交。

这里假设标准模板R为字母ABCDEF(6个)，测试模板T为1234(4个)。R和T中各元素之间的距离已经给出。如下：

![](/images/img2/DTW.png)




更一般的，DTW也可用于计算两个离散的序列(不一定要与时间有关)的相似度。

参考：

http://blog.csdn.net/zouxy09/article/details/9140207

动态时间规整（DTW）

https://blog.csdn.net/raym0ndkwan/article/details/45614813

DTW动态时间规整

http://www.cnblogs.com/tornadomeet/archive/2012/03/23/2413363.html

从一个实例中学习DTW算法

http://www.cnblogs.com/luxiaoxun/archive/2013/05/09/3069036.html

Dynamic Time Warping动态时间规整算法

# Pitch Detection

http://blog.csdn.net/zouxy09/article/details/9141875

基音周期估计（Pitch Detection）

# Vector Quantization

http://blog.csdn.net/zouxy09/article/details/9153255

矢量量化（Vector Quantization）

# MFCC

## Mel scale

Mel scale是Stevens、Volkmann和Newman于1937年发明的一种主观音阶标准。

>Stanley Smith Stevens，1906～1973，Harvard University心理学教授。

>John E. Volkmann，1905～1980，Radio Corporation of America研究员。

>Edwin B. Newman，1908~1989，Harvard University心理学教授。

声音作为一种波动，一般以Hz作为频率差异的客观标准，然而相同频率差的两组声音，在人耳听来，其频率差（也就是所谓的音阶）实际上是不同的。因此，Stevens等人采取实验的方法，确定了人耳的主观音阶标准。

该标准以Mel作为单位，规定1000Hz的声音所对应的音阶为1000Mel。

Mel scale从严格的定义上并没有一个简单的公式来表示。但一般采用如下公式进行转换：

$$m = 2595 \log_{10}\left(1 + \frac{f}{700}\right)$$

从中可以看出，人耳对于高频声音的分辨率实际上是不如低频声音的。

![](/images/img2/Mel_scale.png)

>Mel是melody的别称，有的blog上说Mel是个人，他发明了MFCC，这纯粹是胡说八道。

## MFCC

Mel-frequency cepstral coefficients是由Paul Mermelstein提出的一种音频特征。

>Paul G. Mermelstein，明尼苏达大学神经科学教授。

## 参考

http://blog.csdn.net/zouxy09/article/details/9156785

梅尔频率倒谱系数（MFCC）

https://my.oschina.net/jamesju/blog/193343

语音特征参数MFCC提取过程详解

https://liuyanfeier.github.io/2017/10/26/2017-10-27-Kaldi%E4%B9%8Bfbank%E5%92%8Cmfcc%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/

kaldi之fbank和mfcc特征提取

https://zhuanlan.zhihu.com/p/26680599

语音信号预处理及特征参数提取

