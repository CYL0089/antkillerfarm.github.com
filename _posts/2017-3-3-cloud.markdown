---
layout: post
title:  云计算论文集, Spark, 数据描述语言
category: AI 
---

# 云计算论文集

这里列出一些在这个领域产生重大影响的论文。仅作备忘，肯定不全，Google是其中的绝对主力。

## CAP

《Towards Robust Distributed Systems》（2000）

>作者：Eric Brewer是University of California, Berkeley的计算机科学教授，在Google担任基础设施方面的VP。他的研究兴趣包括云计算、可伸缩的服务器、传感器网络，还有适合发展中地区应用的技术。他还帮助建立了美国联邦政府的门户网站USA.gov。Brewer从MIT获得电子工程和计算机科学的博士学位。他是National Academy of Engineering的院士。

## GFS

《The Google File System》（2003）

开源实现：Hadoop HDFS

>作者：Sanjay Ghemawat，Ph.D. in Computer Science from MIT，2009年当选美国国家工程学院院士。

>Howard Gobioff(1971 – 2008)，Carnegie Mellon University，Ph.D. Computer Science。

>Shun-Tak Leung，University of Washington，Ph.D. Computer Science。

## MapReduce

《MapReduce: Simplified Data Processing on Large Clusters》（2004）

开源实现：Hadoop MapReduce

>作者：Jeffrey Dean，University of Washington，Ph.D. Computer Science (1996)，2009年当选美国国家工程学院院士。

>Sanjay Ghemawat。

## Bigtable

开源实现：Hbase

《Bigtable: A Distributed Storage System for Structured Data》（2006）

>作者：Fay W. Chang，Carnegie Mellon University，Ph.D. Computer Science 。

>Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber

## Spanner

《Spanner: Google’s Globally-Distributed Database》（2012）

>作者：James C. Corbett，PhD in Computer Science from the University of Massachusetts at Amherst.

>Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh,Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura,David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi Saito, Michal Szymaniak,Christopher Taylor, Ruth Wang, Dale Woodford

## Dremel

Dremel是Google的“交互式”数据分析系统。可以组建成规模上千的集群，处理PB级别的数据，是MapReduce的有力补充。

《Dremel: Interactive Analysis of WebScaleDatasets》（2006）

>作者：Sergey Melnik, Ph.D. in Computer Science from the University of Leipzig, Germany. Microsoft Research (2003-2008), Google(since 2008)

>Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis

## Chubby Lock

《The Chubby lock service for loosely-coupled distributed systems》（2006）

开源实现：Zookeeper

>作者：Mike Burrows，1963年生，英国计算机学家。剑桥大学博士。2013年当选皇家学会会员。

# Spark

官网：

http://spark.apache.org/

## RDD

Resilient Distributed Datasets是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。RDD克服了传统的MapReduce所进行大量的磁盘IO操作。

我的理解：RDD将计算包括中间结果，全部放到内存中，以节省磁盘IO操作。

http://www.infoq.com/cn/articles/spark-core-rdd

理解Spark的核心RDD

http://f.dataguru.cn/thread-475874-1-1.html

Spark RDD详解

## DataFrame和DataSet

http://www.jianshu.com/p/c0181667daa0

RDD、DataFrame和DataSet的区别

http://www.csdn.net/article/2015-02-17/2823997

Spark新年福音：一个用于大规模数据科学的API——DataFrame

https://www.iteblog.com/pdf/1675

Spark 2.0介绍：从RDD API迁移到DataSet API

## transformation & action

transformation是得到一个新的RDD，方式很多，比如从数据源生成一个新的RDD，从RDD生成一个新的RDD。

action是得到一个值，或者一个结果（直接将RDDcache到内存中）。

所有的transformation都是采用的懒策略，就是如果只是将transformation提交是不会执行计算的，计算只有在action被提交的时候才被触发。

## Spark部署

Spark没有服务程序，因此无须部署，只需要在集群中的某台机器上安装spark，进行任务提交即可。Spark的并行执行主要依赖Hadoop YARN。

参见：

http://blog.csdn.net/book_mmicky/article/details/25714287

Spark虽然对Hadoop的版本有一定的要求，但是并不是太严重的问题。比如，目前最新的Spark 2.0.1（2016.10）仍然支持Hadoop 2.3，而后者是2014年2月出的版本。

## shuffle

![](/images/article/mapreduce-process.jpg)

上图描述了MapReduce算法的整个流程，其中shuffle phase就是介于Map phase和Reduce phase之间的那一堆连线。很显然，shuffle虽然是MapReduce算法提出的概念，但在各类分布式计算框架中普遍存在，也是影响计算效率的关键点和框架设计的难点之一。

参考：

http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/

## 控制日志输出等级

有的时候为了防止控制台的日志输出过多，淹没了程序的正常输出，可以采用如下方法：

{% highlight java %}
SparkContext sc = SparkContext.getOrCreate(conf);
sc.setLogLevel("WARN");
{% endhighlight %}

## 基本统计操作

1.统计列中不相同值的个数。

方法一：

比如在观众观影的表格中，找出观众的数量或电影的数量。

`Dataset<Row> df = session.sql("SELECT userId FROM Movie group by userId");//df.count() is the count of different values`

方法二：

Spark中有个org.apache.spark.sql.functions类，专门针对数据集进行各种运算操作。其中的countDistinct方法可实现该功能。片段示例如下：

`Dataset<Row> df2 = df.agg(functions.countDistinct("userId"));df2.show();`

## MLlib

MLlib是Spark的机器学习库。官网：

https://spark.apache.org/mllib/

和Spark的其他部分一样，MLlib也存在RDD API和DataFrame API两套API。其中前者已经停止开发，而后者的设计思想显然来自R语言。RDD API在spark.mllib包中，而DataFrame API在spark.ml包中。

## 参考

http://www.cnblogs.com/zlslch/p/5723857.html

Spark的各种算子

http://mp.weixin.qq.com/s?__biz=MjM5NzAyNTE0Ng==&mid=2649517135&idx=2&sn=7fc02a006b7c5015f3492354d0e298a4&scene=0#rd

Spark性能优化指南：高级篇

https://www.zhihu.com/question/23079001

内存有限的情况下，Spark如何处理T级别的数据

https://www.zhihu.com/question/26568496

与Hadoop对比，如何看待Spark技术？

http://www.csdn.net/article/1970-01-01/2825748

如何利用“图计算”实现大规模实时预测分析

http://www.cnblogs.com/bluejoe/p/5115846.html

学习GraphX

http://www.cnblogs.com/bluejoe/p/5115845.html

Hive体系结构介绍

# 数据描述语言

## JSON

JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。 易于人阅读和编写。同时也易于机器解析和生成。 它基于JavaScript Programming Language, Standard ECMA-262 3rd Edition - December 1999的一个子集。

其官网为：

http://json.org/

官网上列出了各种语言的JSON解析库。其中C语言的解析库中以json-c最为流行，其官网为：

https://github.com/json-c/json-c

和XML Path类似，JSON也定义了自己的JSON Path。参见：

http://goessner.net/articles/JsonPath/

参考：

https://addons.mozilla.org/zh-CN/firefox/addon/jsonview/

jsonview是一个用于检验JSON格式是否合法的Firefox插件。

http://www.bejson.com/

一个在线验证JSON语法的库。

https://docs.python.org/2/library/json.html

python自带的json包的文档

## Jackson

Jackson是常用的Java语言的JSON库。

Maven安装：

{% highlight text %}
<dependency>
	<groupId>com.fasterxml.jackson.core</groupId>
	<artifactId>jackson-databind</artifactId>
	<version>2.6.4</version>
</dependency>
{% endhighlight %}

Jackson提供了三种处理方法：

**流式API**：（也称为"增量分析/生成"）读取和写入 JSON 内容作为离散事件。类似于XML SAX。

com.fasterxml.jackson.JsonParser读，com.fasterxml.jackson.JsonGenerator写。

**树模型**：提供一个JSON文档可变内存树的表示形式。类似于XML DOM。

com.fasterxml.jackson.databind.ObjectMapper生成树；树组成JsonNode节点集。

**数据绑定**：JSON和POJO相互转换，基于属性访问器规约或注解。

Jackson不支持JSON Path，可以使用以下项目：

https://github.com/jayway/JsonPath

这个项目的后端可以选择Jackson或Gson。

## BSON

Binary JSON是在JSON的基础上，添加了索引及数据类型的一种二进制格式。相比JSON，它牺牲了可阅读性，得到了可遍历性和高效性。

BSON最早由MongoDB项目提出并使用，它的官网为：

http://bsonspec.org/

从中可以看出大多数语言的BSON解析库，都是MongoDB项目提供的。

## YAML

YAML(Yet Another Markup Language)是JSON的超集。它没有JSON那么流行，主要被用于科学计算领域，比如OpenCV项目。它的官网为：

http://yaml.org/

这个网站很有特色，它本身就是一个YAML文件。

## Protocol Buffers

Protocol Buffers是Google公司开发的一种数据描述语言。它的官网为：

https://github.com/google/protobuf

文档：

https://developers.google.com/protocol-buffers/

Tutorials：

https://developers.google.com/protocol-buffers/docs/tutorials

安装：

`sudo apt install protobuf-compiler python-protobuf`

这是一种注重效率，而可阅读性几乎为零的二进制格式。其效率超过BSON，但除非有相关格式文件，否则完全无法阅读。而BSON作为JSON的扩展，只有扩展的那部分不可读，其余部分仍保留JSON的可读性。

格式文件是Protocol Buffers中的重要概念，也是和JSON等格式在使用思路上最大的区别。

JSON采用的是，不同的语言提供不同的库来解析的方式。

而Protocol Buffers使用同一个格式文件，为不同语言生成相应的代码。这和CORBA的做法很类似。

与Protocol Buffers类似的方案，还有Facebook提出的Thrift、ZeroC提出的Slice和Hadoop Avro。

### 简易示例

test.proto:

{% highlight text %}
message TestMsg
{
    required int32 id=1;
    required int32 time=2;
    optional string note=3;
}
{% endhighlight %}

`protoc --python_out=./python/ test.proto`

test.py:

{% highlight python %}
import google.protobuf
from test_pb2 import TestMsg
import time

test = TestMsg()
test.id=1
test.time=int(time.time())
test.string="asdftest"
print test
test_str = test.SerializeToString()
print test_str

test1 = TestMsg()
test1.ParseFromString(test_str)
print test1
{% endhighlight %}

示例代码下载：

https://github.com/antkillerfarm/antkillerfarm_crazy/tree/master/helloworld/protobuf

参考：

http://www.cnblogs.com/o87481299/p/4199892.html

python google protobuf使用



